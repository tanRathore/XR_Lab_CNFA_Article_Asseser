./
    README.md:
        
--------------------
    project_explorer.py:
        import os
        import fnmatch
        
        # Maximum number of characters from each file to display
        MAX_CHARACTERS = 500000000
        
        def display_project_structure_with_content(startpath='.'):
            # Directories to exclude from traversal
            excluded_dirs = ['.git', '__pycache__']
        
            # File patterns considered text files
            text_file_patterns = ['*.py', '*.json', '*.yaml', '*.yml', '*.md','*.ipynb']
        
            for root, dirs, files in os.walk(startpath):
                # Exclude certain directories
                dirs[:] = [d for d in dirs if d not in excluded_dirs]
        
                # Determine the indentation based on directory depth
                level = root.replace(startpath, '').count(os.sep)
                indent = ' ' * 4 * level
                # Print directory name
                print('{}{}/'.format(indent, os.path.basename(root)))
        
                subindent = ' ' * 4 * (level + 1)
                for filename in files:
                    # Check if the file matches any of our text patterns
                    if any(fnmatch.fnmatch(filename, pattern) for pattern in text_file_patterns):
                        filepath = os.path.join(root, filename)
                        print(f"{subindent}{filename}:")
                        try:
                            with open(filepath, 'r', encoding='utf-8') as f:
                                content = f.read()
                                # not Limiting the content length
                                shortened_content = content
                                # Remove leading/trailing whitespace and print line by line
                                stripped_content = shortened_content.strip()
                                content_lines = stripped_content.split('\n')
                                for line in content_lines:
                                    print(f"{subindent*2}{line}")
                        except UnicodeDecodeError:
                            print(f"{subindent*2}[Binary or undecodable file]")
                        except Exception as e:
                            print(f"{subindent*2}[Error reading file: {e}]")
                        print("-" * 20)  # Separator between files
        
        if __name__ == "__main__":
            display_project_structure_with_content()
--------------------
    vector_store_data/
        chroma_db_cna/
            c153e016-2cfd-48e1-b828-3a1a94a004a5/
            fdffe17a-c940-4eda-b0a4-6ad0d3c17f33/
            3eac4fef-b7f1-4970-8709-b45023820785/
            2b574469-3177-4465-9935-4ddcaa446100/
            0ccf013c-0a87-4f66-9566-dab8aac08767/
            7c88ff16-5631-4fc8-b2de-1b63b3dd4611/
            f73a42a3-4a09-4e17-a1f8-7cae6a761db2/
            0be9dbb8-9533-4e1e-9465-ae763c70180b/
            6e18dc1c-8ae6-4351-ac4c-43a3d88fa34a/
            8d66d9dc-db00-4515-90be-b8b63e862698/
            1134d418-4da7-4fb1-91c1-d2e77779453f/
            7991ea13-a97a-422e-8089-e3714ec5fcd5/
            757212d1-3b62-4550-9f79-fc4d2eb56ba6/
            c05144d8-830a-45be-bd16-328362b31dc9/
            3b6480b8-f74a-4e95-a8d3-9dfe9656584c/
            1f46aa9e-afd8-40b2-b9be-7bfe7073cadd/
            d4ffe21d-a07d-460a-9828-b3560ef1e42c/
    tests/
        __init__.py:
                
--------------------
        test_data_ingestion.py:
                
--------------------
    data/
        manual_links.json:
                {
                    "EmbodiedCognition.pdf": [
                      "A. Environmental Priming → Creative Task",
                      "E. Lighting–Sound Congruence → Emotional Coherence"
                    ]
                }
--------------------
        raw_documents/
        processed_data/
            EmbodiedCognition_3pager_summary.md:
                        ## Embodied Cognition and the Magical Future of Interaction Design: A Summary
                        
                        **1. Introduction & Problem Statement:**
                        
                        This article explores the implications of embodied cognition theory for human-computer interaction (HCI) design, particularly for tangible, physical, context-aware, and telepresence systems. It argues that our interaction with tools fundamentally changes how we think and perceive, extending beyond mere utility to reshape our cognitive processes. The central problem addressed is how to leverage this understanding of embodied cognition to design more effective and intuitive interactive systems.
                        
                        **2. Key Methodologies:**
                        
                        The article primarily employs a conceptual analysis approach, drawing on existing research in cognitive science, neuroscience, and psychology to support its claims. It integrates findings from studies on tool use, perception, and motor resonance to build a theoretical framework for embodied interaction.  A small empirical study on dance practice is also presented.  Mathematical formalisms are not central, but the concept of a "continuation system" (a lattice of predicted outcomes based on actions) is used to model how interaction with the world shapes expectations and inferences.
                        
                        **3. Core Findings & Results:**
                        
                        * **Tool Absorption:** Tools are not merely external objects but become integrated into our body schema, altering our perception and conception of the environment.  This changes our peripersonal space and the affordances we perceive.
                        * **Embodied Thinking:**  Thinking is not confined to the brain but is distributed across the body and environment. Physical actions, such as gestures and manipulations, can be integral parts of the thinking process.
                        * **Marking as a Cognitive Strategy:**  "Marking" in dance (practicing a simplified or distorted version of a movement) was found to be a more effective learning strategy than mental simulation or even full-out practice for certain aspects of movement. This suggests that the body can be used as a physical modeling tool to support cognitive processes.
                        * **Limitations of Observation:** While motor resonance allows us to learn by observing others, direct physical interaction provides unique kinesthetic information crucial for certain types of learning and creative exploration.
                        
                        **4. Detailed Experimental Setup / Stimuli Duplication (if applicable):**
                        
                        The article details one experiment:
                        
                        * **Participants:** 10 superexpert dancers from Random Dance company.
                        * **Procedure:**
                            1. **Teaching Phase:** Dancers were taught a new dance phrase (approx. 55 seconds) for 10 minutes.
                            2. **Baseline Measurement:** Each dancer performed the phrase individually and was graded by the teacher.
                            3. **Practice Phase:** Dancers were divided into three groups and practiced the phrase for 10 minutes in one of three conditions: (a) Full-out practice, (b) Marking, (c) Mental simulation (lying on their back). Dancers faced different directions and were instructed not to look at each other.
                            4. **Final Measurement:**  Dancers were individually graded again on their performance.
                            5. **Repeated Measures:**  The entire process was repeated two more times with new phrases, with each dancer experiencing all three practice conditions. The order of conditions was balanced using a Latin square design.
                        * **Measures:** Performance was graded on a 5-point scale (increments of 0.5) across four dimensions: (a) Technicality (precision of positions and transitions), (b) Memory (completeness of movement), (c) Timing (precision of durations), (d) Dynamics (force, speed, acceleration, and qualitative aspects of movement).  Grading was done by the teacher in real-time and later by two independent expert observers reviewing video recordings frame-by-frame.
                        * **Stimuli:**  The specific dance phrases used are not described in detail, making precise duplication difficult.  The article mentions they were of similar duration (approx. 55 seconds) and complexity.
                        
                        **5. Discussion & Implications:**
                        
                        The findings suggest that embodied cognition principles can inform the design of interactive systems by:
                        
                        * **Leveraging tool absorption:** Designing tools that seamlessly integrate with our body schema can enhance perception and action.
                        * **Supporting embodied thinking:**  Creating interfaces that allow for physical manipulation and gesture can facilitate cognitive processes.
                        * **Facilitating marking-like strategies:**  Providing opportunities for users to interact with simplified models or simulations can improve learning and skill acquisition.
                        * **Recognizing the limitations of observation:**  While observation can be helpful, designers should prioritize opportunities for direct physical interaction when kinesthetic feedback is crucial.
                        
                        **Limitations:** The dance study involved a small, specialized sample (expert dancers). The generalizability of the findings to other domains and skill levels requires further investigation. The article acknowledges the need for more research to determine the specific conditions under which marking and other embodied strategies are most effective.
                        
                        **Future Work:**  Further research is needed to explore the limits of tool absorption, the cognitive benefits of different types of physical interaction, and the design principles for effective embodied interfaces.
                        
                        **6. Key Figures/Tables to Reference (Optional but helpful):**
                        
                        * **Figure 1:** Illustrates "hand marking" in dance.
                        * **Figure 2:** Depicts the three experimental conditions (full-out, marking, mental simulation).
                        * **Figure 3:** Shows the temporal structure and experimental design.
                        * **Table II:** Summarizes the mean improvement from practice in each condition.
                        * **Table III:** Shows learning broken down by dimension (memory, technicality, timing, dynamics).
                        * **Table IV:** Presents p-values for the significance of findings.  (Note: There are two tables labeled "Table IV" in the original article.  The summary refers to the one presenting p-values for the dance experiment.)
--------------------
            summaries/
                dummy_loader_test_3pager_summary.md:
                                **Summary of "Simple Test Document"**
                                
                                **1. Introduction & Problem Statement:**
                                
                                The provided document is a "simple test document" explicitly stated for testing purposes. It does not present a scientific problem or context.  No research question or hypothesis is defined.
                                
                                **2. Key Methodologies:**
                                
                                The document does not describe any methodologies, approaches, or techniques. It consists of two short paragraphs.
                                
                                **3. Core Findings & Results:**
                                
                                The document does not present any findings, results, or data.  It solely serves as a test for an unspecified loader or process.
                                
                                **4. Detailed Experimental Setup / Stimuli Duplication (if applicable):**
                                
                                Experimental/stimuli duplication details are not provided or not applicable.  The document does not describe any experiments or procedures.
                                
                                **5. Discussion & Implications:**
                                
                                The document does not offer any discussion, implications, limitations, or suggestions for future work.  There are no scientific claims or conclusions to discuss.
                                
                                **6. Key Figures/Tables to Reference (Optional but helpful):**
                                
                                There are no figures or tables in the provided document.
--------------------
                EmbodiedCognition_3pager_summary.md:
                                ## Embodied Cognition and the Magical Future of Interaction Design: A Summary
                                
                                **1. Introduction & Problem Statement:**
                                
                                This article explores the implications of embodied cognition theory for human-computer interaction (HCI) design, particularly for tangible, physical, context-aware, and telepresence systems.  It argues that traditional views of cognition, which separate mind from body and environment, are insufficient for designing effective interactive systems. The core problem addressed is how to leverage the principles of embodied cognition to create more intuitive and powerful interactive experiences.
                                
                                **2. Key Methodologies:**
                                
                                The article primarily uses a conceptual and theoretical approach, drawing on existing research in cognitive science, neuroscience, and psychology. It combines literature review with a case study of expert dancers and analysis of choreographic creation.  No specific mathematical formalisms are central to the arguments.
                                
                                **3. Core Findings & Results:**
                                
                                * **Tool Absorption:** Tools, when mastered, become integrated into the body schema, altering perception, conception, and action. This changes the user's "enactive landscape," the perceived environment structured by potential actions.
                                * **Thinking with the Body:**  A study of expert dancers revealed that "marking" (practicing a simplified, distorted version of a dance phrase) is more effective for learning than mental simulation or even full-out practice in some aspects. This suggests the body acts as a physical modeling and simulation device for cognitive processes.
                                * **Interacting vs. Observing:**  Choreographers gain more understanding by physically enacting a dance movement than by observing it, suggesting that kinesthetic feedback provides crucial information not available visually.
                                * **Thinking with Things:**  The concept of embodied cognition is extended to include interaction with external objects. Manipulating objects can be a form of external simulation that drives internal cognitive processes.
                                
                                **4. Detailed Experimental Setup / Stimuli Duplication (if applicable):**
                                
                                The article details one experiment:
                                
                                * **Participants:** 10 superexpert dancers from Random Dance company.
                                * **Procedure:**
                                    1. **Teaching Phase:** Dancers were taught a new dance phrase (approx. 55 seconds) for 10 minutes.
                                    2. **Baseline Measurement:** Each dancer performed the phrase individually and was graded by the teacher.
                                    3. **Practice Phase:** Dancers were divided into three groups, each practicing the phrase for 10 minutes in one of three conditions: (a) full-out practice, (b) marking, (c) mental simulation (lying on their back). Dancers faced different directions and were instructed not to look at each other.
                                    4. **Final Measurement:**  Dancers were individually re-evaluated on their performance.
                                    5. **Repetition:** Steps 1-4 were repeated two more times, with dancers rotating through the practice conditions (Latin Square design).
                                * **Stimuli:**  Two new, complex dance phrases of approximately equal duration and complexity. Specific details of the phrases are not provided.
                                * **Measures:** Performance was graded on a 5-point scale (increments of 0.5) across four dimensions: (a) technicality (precision of positions and transitions), (b) memory (completeness of movement), (c) timing (precision of durations), (d) dynamics (force, speed, acceleration, and qualitative aspects of motion).  Grading was done by the teacher in real-time and later by two independent expert observers reviewing video recordings frame-by-frame.
                                * **Data Analysis:** One-way ANOVAs and post-hoc pairwise comparisons (Tukey's HSD) were used to analyze the differences in performance improvement (learning delta) between practice conditions.
                                
                                The tic-tac-toe experiment is described in less detail and lacks specific information on the number of participants, board size used, and the exact procedure for the imagination condition.
                                
                                **5. Discussion & Implications:**
                                
                                The findings suggest that embodied cognition principles can inform the design of interactive systems by:
                                
                                * **Leveraging Tool Absorption:** Designing tools that seamlessly integrate with the body schema can enhance user experience and performance.
                                * **Facilitating Embodied Simulation:**  Providing opportunities for users to physically interact with simplified models or representations can improve learning and understanding.
                                * **Considering Multisensory Input:**  Recognizing the unique contributions of different sensory modalities, particularly kinesthetic feedback, is crucial for designing effective interactive experiences.
                                
                                Limitations of the study include the small sample size in the dance experiment and the lack of detailed information on the tic-tac-toe experiment. Future work could explore the generalizability of these findings to other domains and investigate the specific mechanisms underlying the benefits of embodied interaction.
                                
                                
                                **6. Key Figures/Tables to Reference (Optional but helpful):**
                                
                                * **Figure 1:** Illustrates "hand marking" in dance.
                                * **Figure 2:** Shows the three experimental conditions for dance practice.
                                * **Figure 3:** Depicts the temporal structure and experimental design.
                                * **Table II:** Summarizes the mean improvement from practice in each condition.
                                * **Table III:** Shows learning broken down by dimension (memory, technicality, timing, dynamics).
                                * **Table IV:** Presents p-values for the significance of findings in the dance experiment and mean time per move in the tic-tac-toe experiment.
--------------------
                dummy_test_doc_3pager_summary.md:
                                ## Summary of "Simple Test Document"
                                
                                **1. Introduction & Problem Statement:**
                                
                                The provided document does not explicitly state a problem or context. It serves as a simple test, containing two paragraphs.  No research question or hypothesis is presented.
                                
                                **2. Key Methodologies:**
                                
                                The document does not describe any methodologies.  It is purely textual and presents no experimental design, analytical approach, or theoretical framework.
                                
                                **3. Core Findings & Results:**
                                
                                The document does not present any findings or results. It consists of two paragraphs without specific content related to scientific investigation.
                                
                                **4. Detailed Experimental Setup / Stimuli Duplication (if applicable):**
                                
                                Experimental/stimuli duplication details are not provided or not applicable.  The document does not describe any experiments or stimuli.
                                
                                **5. Discussion & Implications:**
                                
                                The document does not offer any discussion, implications, limitations, or suggestions for future work.  There is no analysis or interpretation of results as no results are presented.
                                
                                **6. Key Figures/Tables to Reference (Optional but helpful):**
                                
                                No figures or tables are present in the document.
--------------------
    .cache/
        cache_metadata.json:
                {
                    "EmbodiedCognition.pdf": 1747089883.38252,
                    "dummy_test_doc.txt": 1747089934.1232333,
                    "BEST_Handbook of CNfA Experimental Paradigms.docx": 1755119652.3366082,
                    "dummy_loader_test.txt": 1747091238.0166535,
                    "manual_links.json": 1755200627.4921076
                }
--------------------
    notebooks/
        05_summary_generation_dev.ipynb:
                
--------------------
    src/
        streamlit_app.py:
                import streamlit as st
                import time
                from langchain.chains import RetrievalQA # For type hinting
                
                # --- Page Configuration - MUST BE THE FIRST STREAMLIT COMMAND ---
                st.set_page_config(page_title="CNfA RAG Agent", layout="wide", initial_sidebar_state="expanded")
                
                try:
                    from cna_rag_agent.utils.logging_config import logger
                    from cna_rag_agent import config
                    from cna_rag_agent.pipeline.rag_pipeline import setup_qa_chain #MODIFIED IMPORT
                except ImportError:
                    # ... (fallback import logic as before, ensure it imports setup_qa_chain) ...
                    import sys
                    from pathlib import Path
                    SRC_DIR = Path(__file__).resolve().parent
                    if str(SRC_DIR) not in sys.path: sys.path.insert(0, str(SRC_DIR))
                    PROJECT_ROOT = SRC_DIR.parent
                    if str(PROJECT_ROOT) not in sys.path: sys.path.insert(0, str(PROJECT_ROOT))
                    from cna_rag_agent.utils.logging_config import logger
                    from cna_rag_agent import config
                    from cna_rag_agent.pipeline.rag_pipeline import setup_qa_chain
                
                
                st.title("📚 CNfA RAG Agent")
                cache_mod_time_str = "N/A - Cache metadata not found"
                if hasattr(config, 'CACHE_METADATA_FILE') and config.CACHE_METADATA_FILE.exists():
                    try: cache_mod_time_str = time.ctime(config.CACHE_METADATA_FILE.stat().st_mtime)
                    except Exception as e: logger.warning(f"Could not get cache mod time: {e}"); cache_mod_time_str = "N/A - Error"
                st.caption(f"Your intelligent assistant for CNfA docs. Last data processed: {cache_mod_time_str}")
                
                # --- Sidebar & Initial Checks ---
                st.sidebar.header("Info & Controls")
                pipeline_ready = True
                if not config.GOOGLE_API_KEY:
                    st.sidebar.error("CRITICAL: GOOGLE_API_KEY is not configured. Q&A will not work.")
                    pipeline_ready = False
                
                # Check ChromaDB status
                try:
                    from cna_rag_agent.vector_store.store_manager import get_persistent_client, CHROMA_COLLECTION_NAME
                    client = get_persistent_client()
                    collection = client.get_collection(name=CHROMA_COLLECTION_NAME)
                    db_doc_count = collection.count()
                    st.sidebar.metric(label="Documents in Vector Store", value=db_doc_count)
                    if db_doc_count == 0:
                        st.sidebar.warning("Vector store is empty. Please run ingestion: `python src/main.py ingest --clear-store`")
                        pipeline_ready = False
                except Exception as e:
                    st.sidebar.warning(f"Could not query vector store status: {e}. Ingestion may be needed.")
                    pipeline_ready = False
                
                if pipeline_ready:
                    st.sidebar.success("System appears ready.")
                else:
                    st.sidebar.error("System not fully ready. Check messages and logs.")
                
                # Two-step retrieval options (can be moved to advanced settings later)
                k_articles_select = st.sidebar.slider("Number of articles to pre-select (Step 1):", 1, 10, 3)
                # Define categories for article pre-selection. These should match categories from `unstructured`
                # that typically represent high-level content like titles, abstracts, or important leading paragraphs.
                # This might need tuning based on your documents.
                article_filter_categories_default = ["Title", "Abstract", "Introduction", "Header", "NarrativeText"]
                # Using a multiselect for categories for more control in the future, or keep as fixed list for now
                # selected_filter_categories = st.sidebar.multiselect(
                #     "Categories for Article Pre-selection:",
                #     options=["Title", "Abstract", "Introduction", "Header", "NarrativeText", "ListItem", "UncategorizedText"],
                #     default=article_filter_categories_default
                # )
                
                st.info("Welcome! Ask questions about the CNfA document corpus.")
                
                # --- Chat Interface ---
                if "messages" not in st.session_state:
                    st.session_state.messages = [{"role": "assistant", "content": "How can I help you find information in the documents?"}]
                
                for msg in st.session_state.messages:
                    with st.chat_message(msg["role"]):
                        st.write(msg["content"])
                
                if prompt := st.chat_input("Ask your question here..."):
                    st.session_state.messages.append({"role": "user", "content": prompt})
                    with st.chat_message("user"):
                        st.write(prompt)
                    
                    logger.info(f"Streamlit App: User query: {prompt}")
                
                    if not pipeline_ready:
                        error_message = "The system is not ready for queries. Please check the sidebar messages and console logs."
                        st.error(error_message)
                        st.session_state.messages.append({"role": "assistant", "content": error_message})
                        with st.chat_message("assistant"): st.write(error_message)
                    else:
                        try:
                            with st.spinner("Performing two-step retrieval and generating answer..."):
                                # Setup the QA chain for this specific query (enables two-step)
                                # Using default categories for now, can use selected_filter_categories if multiselect is added
                                current_qa_chain = setup_qa_chain(
                                    user_query=prompt, # This triggers the first step article selection
                                    k_articles_for_selection=k_articles_select,
                                    article_filter_categories=article_filter_categories_default
                                )
                                
                                if not current_qa_chain:
                                    error_message = "Could not set up the QA chain for your query. This might be due to issues finding relevant articles or an internal error."
                                    st.error(error_message)
                                    logger.error(f"Streamlit App: Failed to setup_qa_chain for query '{prompt}'.")
                                    st.session_state.messages.append({"role": "assistant", "content": error_message})
                                    with st.chat_message("assistant"): st.write(error_message)
                                else:
                                    response = current_qa_chain.invoke({"query": prompt}) # Pass the same query to the final chain
                                    result_text = response.get("result", "Sorry, I could not find an answer or an error occurred.")
                                    source_docs = response.get("source_documents", [])
                            
                            st.session_state.messages.append({"role": "assistant", "content": result_text})
                            with st.chat_message("assistant"):
                                st.write(result_text)
                                if source_docs:
                                    with st.expander("Show Sources Consulted", expanded=False):
                                        for i, doc in enumerate(source_docs):
                                            source_name = doc.metadata.get('file_name', doc.metadata.get('source', 'N/A'))
                                            page_num_meta = doc.metadata.get('page_number', doc.metadata.get('page', 'N/A'))
                                            category = doc.metadata.get('category', 'N/A')
                                            content_snippet = doc.page_content.strip().replace("\n", " ")[:250] + "..." if doc.page_content else "N/A"
                                            st.markdown(f"**Source {i+1}:** `{source_name}` (Page: {page_num_meta}, Type: `{category}`)")
                                            st.caption(f"> {content_snippet}")
                                elif "cannot answer" not in result_text.lower() and "sorry" not in result_text.lower():
                                     st.caption("Answer generated. To see specific source snippets, the query might need to be more targeted or a different retrieval strategy could be used.")
                            logger.info(f"Streamlit App: LLM Response for '{prompt}': {result_text[:200]}...")
                
                        except Exception as e:
                            logger.error(f"Streamlit App: Error processing query '{prompt}': {e}", exc_info=True)
                            error_display_message = f"An error occurred: {str(e)[:200]}..."
                            st.error(error_display_message)
                            st.session_state.messages.append({"role": "assistant", "content": error_display_message})
                            with st.chat_message("assistant"):
                                st.write(error_display_message)
                
                if st.sidebar.button("Clear Chat History"):
                    st.session_state.messages = [{"role": "assistant", "content": "Chat history cleared. How can I help you?"}]
                    logger.info("Streamlit App: Chat history cleared by user.")
                    st.rerun()
                
                st.sidebar.markdown("---")
                st.sidebar.markdown("CNfA RAG Agent v0.2 (Two-Step Retrieval)")
--------------------
        main.py:
                import argparse
                from cna_rag_agent.utils.logging_config import logger
                from cna_rag_agent import config # Ensures config is loaded
                
                def run_ingestion_pipeline_main(args):
                    logger.info("Attempting to start data ingestion pipeline via main.py...")
                    try:
                        from cna_rag_agent.pipeline.rag_pipeline import run_full_ingestion_pipeline
                        run_full_ingestion_pipeline(
                            clear_vector_store=args.clear_store,
                            force_reprocess=args.force_reprocess # Pass the new flag
                        )
                    except ImportError as e:
                        logger.error(f"Could not import 'run_full_ingestion_pipeline': {e}", exc_info=True)
                    except Exception as e:
                        logger.error(f"An error occurred during the ingestion pipeline execution from main.py: {e}", exc_info=True)
                
                def start_streamlit_app_main():
                    logger.info("Attempting to start Streamlit application via main.py...")
                    try:
                        import subprocess
                        import sys
                        streamlit_app_path = config.BASE_DIR / "src" / "streamlit_app.py"
                
                        if not streamlit_app_path.exists():
                            logger.error(f"Streamlit app file not found at: {streamlit_app_path}")
                            logger.error("Please ensure 'streamlit_app.py' exists in the 'src/' directory.")
                            return
                
                        logger.info(f"Launching Streamlit from: {streamlit_app_path}")
                        process = subprocess.Popen([sys.executable, "-m", "streamlit", "run", str(streamlit_app_path)])
                        process.wait()
                        logger.info("Streamlit application process finished.")
                    except ImportError as e:
                        logger.error(f"Import error while trying to start Streamlit: {e}", exc_info=True)
                    except FileNotFoundError:
                        logger.error("Streamlit command not found. Is Streamlit installed in the environment and in PATH?")
                    except Exception as e:
                        logger.error(f"An error occurred while trying to start the Streamlit app: {e}", exc_info=True)
                
                
                def main():
                    parser = argparse.ArgumentParser(description="CNfA RAG Agent Command Line Interface")
                    subparsers = parser.add_subparsers(dest="command", help="Available commands")
                    subparsers.required = True
                
                    ingest_parser = subparsers.add_parser("ingest", help="Run the data ingestion pipeline.")
                    ingest_parser.add_argument(
                        "--clear-store",
                        action="store_true",
                        help="Clear the existing vector store before ingesting new documents."
                    )
                    ingest_parser.add_argument(
                        "--force-reprocess",
                        action="store_true",
                        help="Force reprocessing of documents, bypassing any existing cache for chunks/embeddings."
                    )
                    ingest_parser.set_defaults(func=run_ingestion_pipeline_main)
                
                    streamlit_parser = subparsers.add_parser("streamlit_app", help="Start the Streamlit UI.")
                    streamlit_parser.set_defaults(func=start_streamlit_app_main)
                
                    args = parser.parse_args()
                    
                    logger.info(f"Executing command: {args.command} with arguments: {vars(args)}")
                    if hasattr(args, 'func'):
                        if args.command == "ingest":
                            args.func(args)
                        else:
                            args.func()
                    else:
                        parser.print_help()
                
                if __name__ == "__main__":
                    logger.info(f"CNfA RAG Agent starting. Project base directory: {config.BASE_DIR}")
                    main()
--------------------
        scripts/
            link_articles.py:
                        # src/scripts/link_articles.py
                        
                        import sys
                        import json
                        from pathlib import Path
                        import time
                        
                        # --- Setup Project Path ---
                        SRC_DIR = Path(__file__).resolve().parent.parent
                        if str(SRC_DIR) not in sys.path:
                            sys.path.insert(0, str(SRC_DIR))
                        # --- End Setup Project Path ---
                        
                        from cna_rag_agent.utils.logging_config import logger
                        from cna_rag_agent.vector_store.store_manager import get_vector_store, get_persistent_client, CHROMA_COLLECTION_NAME
                        from cna_rag_agent.pipeline.rag_pipeline import generate_and_cache_article_summary
                        from cna_rag_agent.generation.generator import get_llm
                        # --- THIS LINE IS THE ONLY CHANGE ---
                        from cna_rag_agent.config import RAW_DOCUMENTS_PATH, GEMINI_FLASH_MODEL_NAME, DATA_DIR
                        
                        from langchain_core.documents import Document
                        from langchain_core.prompts import PromptTemplate
                        
                        
                        def get_all_paradigms_from_db(vector_store) -> list[dict]:
                            logger.info("Fetching all unique paradigm definitions from the vector store...")
                            results = vector_store._collection.get(where={"content_type": "handbook_entry"}, include=["metadatas"])
                            
                            paradigms = {}
                            if results and results['metadatas']:
                                for meta in results['metadatas']:
                                    paradigm_name = meta.get('paradigm_name')
                                    if paradigm_name and "Uncategorized" not in paradigm_name and "General Introduction" not in paradigm_name:
                                        description_chunk = vector_store._collection.get(
                                            where={"$and": [{"paradigm_name": {"$eq": paradigm_name}}, {"paradigm_section": {"$eq": "What is Studied"}}]},
                                            include=["documents"]
                                        )
                                        if description_chunk['documents']:
                                            paradigms[paradigm_name] = description_chunk['documents'][0]
                        
                            logger.info(f"Found {len(paradigms)} unique paradigms to check for links.")
                            return [{"name": name, "description": desc} for name, desc in paradigms.items()]
                        
                        def get_all_articles_from_db(vector_store) -> list[dict]:
                            logger.info("Fetching all unique scientific articles from the vector store...")
                            results = vector_store._collection.get(include=["metadatas"])
                            
                            articles = {}
                            if results and results['metadatas']:
                                for meta in results['metadatas']:
                                    file_name = meta.get('file_name')
                                    if file_name and file_name != "BEST_Handbook of CNfA Experimental Paradigms.docx":
                                        articles[file_name] = True
                                    
                            logger.info(f"Found {len(articles)} unique articles to process.")
                            return [{"file_name": name} for name in articles.keys()]
                        
                        def check_link_with_llm(article_summary: str, paradigm_description: str) -> bool:
                            prompt_template = PromptTemplate.from_template(
                                """You are a research assistant. Based on the article summary and the paradigm description below, does the article concretely investigate or provide a primary example of this experimental paradigm?
                                Your answer MUST be a single word: YES or NO.
                        
                                Article Summary:
                                {summary_text}
                                ---
                                Paradigm Description:
                                {paradigm_description}
                                Answer (YES or NO):"""
                            )
                            llm = get_llm(model_name=GEMINI_FLASH_MODEL_NAME, temperature=0.0)
                            if not llm: return False
                            chain = prompt_template | llm
                            try:
                                response = chain.invoke({"summary_text": article_summary, "paradigm_description": paradigm_description})
                                return "YES" in response.content.strip().upper()
                            except Exception: return False
                        
                        def update_metadata_in_db(collection, file_name: str, new_paradigm_link: str):
                            results = collection.get(where={"file_name": file_name}, include=["metadatas"])
                            if not results['ids']: return
                        
                            chunk_ids = results['ids']
                            existing_metadatas = results['metadatas']
                            
                            updated_metadatas = []
                            for meta in existing_metadatas:
                                if 'linked_paradigm' in meta:
                                    try:
                                        paradigm_list = json.loads(meta['linked_paradigm'])
                                        if not isinstance(paradigm_list, list): paradigm_list = [paradigm_list]
                                    except (json.JSONDecodeError, TypeError):
                                        paradigm_list = [meta['linked_paradigm']]
                                    if new_paradigm_link not in paradigm_list:
                                        paradigm_list.append(new_paradigm_link)
                                    meta['linked_paradigm'] = json.dumps(paradigm_list)
                                else:
                                    meta['linked_paradigm'] = json.dumps([new_paradigm_link])
                                updated_metadatas.append(meta)
                        
                            logger.info(f"Updating {len(chunk_ids)} chunks for '{file_name}' with new link: '{new_paradigm_link}'")
                            collection.update(ids=chunk_ids, metadatas=updated_metadatas)
                        
                        def main():
                            logger.info("--- Starting Knowledge Base Linking Script ---")
                            vector_store = get_vector_store()
                            if not vector_store: return
                        
                            client = get_persistent_client()
                            collection = client.get_collection(name=CHROMA_COLLECTION_NAME)
                        
                            paradigms = get_all_paradigms_from_db(vector_store)
                            articles = get_all_articles_from_db(vector_store)
                        
                            if not paradigms or not articles:
                                logger.error("No paradigms or articles found to link. Ensure both the handbook and other articles have been ingested.")
                                return
                                
                            manual_links = {}
                            manual_links_path = DATA_DIR / "manual_links.json"
                            if manual_links_path.exists():
                                with open(manual_links_path, 'r') as f:
                                    manual_links = json.load(f)
                        
                            total_links_found = 0
                            for article in articles:
                                file_name = article['file_name']
                                logger.info(f"\n--- Processing article: {file_name} ---")
                                
                                links_to_add = []
                                if file_name in manual_links:
                                    links_to_add = manual_links[file_name]
                                    logger.info(f"Found {len(links_to_add)} manual links for '{file_name}'")
                                else:
                                    summary = generate_and_cache_article_summary(file_name=file_name, documents_dir=RAW_DOCUMENTS_PATH)
                                    if summary:
                                        for paradigm in paradigms:
                                            if check_link_with_llm(summary, paradigm['description']):
                                                logger.info(f"AUTO LINK FOUND: '{file_name}' -> '{paradigm['name']}'")
                                                links_to_add.append(paradigm['name'])
                                            time.sleep(2)
                                
                                if links_to_add:
                                    for link in links_to_add:
                                        total_links_found += 1
                                        update_metadata_in_db(collection=collection, file_name=file_name, new_paradigm_link=link)
                                    
                            logger.info(f"\n--- Linking Script Finished ---")
                            logger.info(f"Total links established/verified: {total_links_found}")
                        
                        if __name__ == "__main__":
                            main()
--------------------
            debug_metadata.py:
                        # src/scripts/debug_metadata.py
                        
                        import sys
                        from pathlib import Path
                        
                        # --- Setup Project Path ---
                        SRC_DIR = Path(__file__).resolve().parent.parent
                        if str(SRC_DIR) not in sys.path:
                            sys.path.insert(0, str(SRC_DIR))
                        # --- End Setup Project Path ---
                        
                        from cna_rag_agent.utils.logging_config import logger
                        from cna_rag_agent.vector_store.store_manager import get_vector_store
                        
                        def inspect_article_metadata(file_name: str):
                            """
                            Connects to the DB and prints the full metadata for the first chunk
                            of a specific article file to diagnose linking issues.
                            """
                            logger.info(f"Connecting to vector store to inspect metadata for article: '{file_name}'")
                            vector_store = get_vector_store()
                            if not vector_store:
                                logger.error("Could not connect to vector store.")
                                return
                        
                            collection = vector_store._collection
                            
                            # Get the first document matching the file name
                            results = collection.get(
                                where={"file_name": file_name},
                                limit=1, # We only need to see one example
                                include=["metadatas"]
                            )
                            
                            if not results or not results['ids']:
                                logger.error(f"Could not find any documents for file_name='{file_name}'.")
                                return
                        
                            logger.info(f"Found chunk for '{file_name}'. Inspecting its full metadata...\n")
                            
                            # Print the full metadata dictionary for the first chunk found
                            first_metadata = results['metadatas'][0]
                            
                            print(f"--- Full Metadata for a chunk from: {file_name} ---")
                            import json
                            # Pretty print the dictionary
                            print(json.dumps(first_metadata, indent=2))
                            print("-" * 50)
                                
                        if __name__ == "__main__":
                            # We want to see the metadata for the article we linked
                            inspect_article_metadata("EmbodiedCognition.pdf")
--------------------
        api/
            server.py:
                        
--------------------
            models.py:
                        
--------------------
            __init__.py:
                        
--------------------
        cna_rag_agent/
            config.py:
                        import os
                        from dotenv import load_dotenv
                        from pathlib import Path
                        import logging # For initial check
                        
                        # Determine the base directory of the project (CNFA_RAG_Agent)
                        BASE_DIR = Path(__file__).resolve().parent.parent.parent
                        ENV_PATH = BASE_DIR / ".env"
                        
                        if not ENV_PATH.exists():
                            print(f"WARNING: '.env' file not found at '{ENV_PATH}'.")
                            print("Please create it based on '.env.example' and add your GOOGLE_API_KEY.")
                        load_dotenv(dotenv_path=ENV_PATH)
                        
                        # --- API Keys ---
                        GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
                        
                        # --- Data Paths ---
                        DATA_DIR = BASE_DIR / "data"
                        RAW_DOCUMENTS_PATH = DATA_DIR / "raw_documents"
                        PROCESSED_DATA_PATH = DATA_DIR / "processed_data"
                        LOG_FILE_PATH = BASE_DIR / "app.log"
                        
                        # --- Vector Store Configuration ---
                        VECTOR_STORE_DIR = BASE_DIR / "vector_store_data"
                        CHROMA_PERSIST_DIR = VECTOR_STORE_DIR / "chroma_db_cna"
                        CHROMA_COLLECTION_NAME = "cna_articles_collection"
                        
                        # --- Embedding Model Configuration ---
                        EMBEDDING_MODEL_NAME = "models/text-embedding-004"
                        EMBEDDING_TASK_TYPE_DOCUMENT = "RETRIEVAL_DOCUMENT"
                        EMBEDDING_TASK_TYPE_QUERY = "RETRIEVAL_QUERY"
                        
                        # --- LLM Configuration (for Gemini via Generative Language API) ---
                        GEMINI_PRO_MODEL_NAME = "gemini-1.5-pro-latest"
                        GEMINI_FLASH_MODEL_NAME = "gemini-1.5-flash-latest"
                        LLM_TEMPERATURE_QA = 0.0
                        LLM_TEMPERATURE_SUMMARIZATION = 0.2
                        LLM_DEFAULT_MAX_OUTPUT_TOKENS = 8192
                        
                        # --- Chunking Configuration ---
                        CHUNK_SIZE_TOKENS = 768
                        CHUNK_OVERLAP_TOKENS = 150
                        TOKENIZER_MODEL_REFERENCE = "gpt-3.5-turbo"
                        
                        # --- Retrieval Configuration ---
                        RETRIEVER_SEARCH_TYPE = "similarity"
                        RETRIEVER_K_RESULTS = 5
                        RETRIEVER_MMR_FETCH_K = 20
                        RETRIEVER_MMR_LAMBDA_MULT = 0.6
                        
                        # --- Logging Configuration ---
                        LOG_LEVEL = "INFO"
                        
                        # --- Cache Configuration --- ADDED THIS SECTION ---
                        CACHE_DIR = BASE_DIR / ".cache" # A common name for cache directories
                        PREPARED_CHUNKS_CACHE_FILE = CACHE_DIR / "prepared_chunks.pkl"
                        EMBEDDINGS_CACHE_FILE = CACHE_DIR / "embedding_vectors.npy"
                        CACHE_METADATA_FILE = CACHE_DIR / "cache_metadata.json"
                        # --- END ADDED SECTION ---
                        
                        # --- Function to create directories if they don't exist ---
                        def create_dir_if_not_exists(dir_path: Path):
                            """Creates a directory if it doesn't already exist."""
                            if not dir_path.exists():
                                dir_path.mkdir(parents=True, exist_ok=True)
                                print(f"INFO: Created directory: {dir_path}")
                        
                        _essential_dirs = [
                            DATA_DIR, RAW_DOCUMENTS_PATH, PROCESSED_DATA_PATH,
                            VECTOR_STORE_DIR, CHROMA_PERSIST_DIR,
                            CACHE_DIR # ADDED CACHE_DIR TO THIS LIST
                        ]
                        for _dir in _essential_dirs:
                            create_dir_if_not_exists(_dir)
                        
                        if not GOOGLE_API_KEY:
                            print("CRITICAL WARNING: GOOGLE_API_KEY is not set in the '.env' file or as an environment variable.")
                            print("The application will not be able to connect to Google Gemini services.")
                        
                        print(f"INFO: Configuration loaded. Project base directory: {BASE_DIR}")
--------------------
            __init__.py:
                        
--------------------
            pipeline/
                __init__.py:
                                
--------------------
                rag_pipeline.py:
                                # src/cna_rag_agent/pipeline/rag_pipeline.py
                                
                                import os
                                import pickle
                                import numpy as np
                                import json
                                import time
                                from typing import List, Optional, Tuple, Dict, Any
                                from pathlib import Path
                                
                                from langchain_core.documents import Document
                                from langchain_core.prompts import PromptTemplate, BasePromptTemplate
                                from langchain_core.vectorstores import VectorStoreRetriever
                                from langchain_core.language_models.chat_models import BaseChatModel
                                from langchain.chains import RetrievalQA
                                
                                try:
                                    from ..config import (
                                        RAW_DOCUMENTS_PATH, PROCESSED_DATA_PATH, DATA_DIR,
                                        PREPARED_CHUNKS_CACHE_FILE, EMBEDDINGS_CACHE_FILE, CACHE_METADATA_FILE,
                                        RETRIEVER_K_RESULTS, RETRIEVER_SEARCH_TYPE,
                                        GEMINI_FLASH_MODEL_NAME, LLM_TEMPERATURE_QA, GOOGLE_API_KEY
                                    )
                                    from ..utils.logging_config import logger
                                    from ..data_ingestion import loader as doc_loader
                                    from ..data_ingestion import preprocessor as doc_preprocessor
                                    from ..embedding import embedder as doc_embedder
                                    from ..vector_store import store_manager as vs_manager
                                    from ..generation import generator as llm_generator
                                    from ..generation import prompts as llm_prompts
                                except ImportError:
                                    import sys
                                    SRC_DIR = Path(__file__).resolve().parent.parent.parent
                                    if str(SRC_DIR) not in sys.path: sys.path.insert(0, str(SRC_DIR))
                                    from cna_rag_agent.config import (
                                        RAW_DOCUMENTS_PATH, PROCESSED_DATA_PATH, DATA_DIR, PREPARED_CHUNKS_CACHE_FILE,
                                        EMBEDDINGS_CACHE_FILE, CACHE_METADATA_FILE, RETRIEVER_K_RESULTS,
                                        RETRIEVER_SEARCH_TYPE, GEMINI_FLASH_MODEL_NAME, LLM_TEMPERATURE_QA, GOOGLE_API_KEY
                                    )
                                    from cna_rag_agent.utils.logging_config import logger
                                    from cna_rag_agent.data_ingestion import loader as doc_loader, preprocessor as doc_preprocessor
                                    from cna_rag_agent.embedding import embedder as doc_embedder
                                    from cna_rag_agent.vector_store import store_manager as vs_manager
                                    from cna_rag_agent.generation import generator as llm_generator, prompts as llm_prompts
                                
                                def get_source_files_metadata(documents_dir: Path) -> Dict[str, float]:
                                    metadata = {}
                                    if documents_dir.exists() and documents_dir.is_dir():
                                        for item in documents_dir.iterdir():
                                            if item.is_file() and not item.name.startswith('.') and item.suffix:
                                                try: metadata[item.name] = item.stat().st_mtime
                                                except Exception as e: logger.warning(f"Could not get stat for file {item.name}: {e}")
                                    manual_links_path = DATA_DIR / "manual_links.json"
                                    if manual_links_path.exists():
                                        metadata["manual_links.json"] = manual_links_path.stat().st_mtime
                                    return metadata
                                
                                def is_cache_valid(documents_dir: Path, cache_metadata_file: Path, chunks_cache_file: Path, embeddings_cache_file: Path) -> bool:
                                    if not all([cache_metadata_file.exists(), chunks_cache_file.exists(), embeddings_cache_file.exists()]): return False
                                    try:
                                        with open(cache_metadata_file, 'r') as f: cached_file_info = json.load(f)
                                        current_file_info = get_source_files_metadata(documents_dir)
                                        if cached_file_info == current_file_info:
                                            logger.info("Cache is valid.")
                                            return True
                                        else:
                                            logger.info("Cache stale: Source document or manual links have changed.")
                                            return False
                                    except Exception: return False
                                
                                def save_to_cache(chunks: List[Document], embeddings: List[List[float]], documents_dir: Path, cache_metadata_file: Path, chunks_cache_file: Path, embeddings_cache_file: Path):
                                    try:
                                        logger.info(f"Saving {len(chunks)} chunks to: {chunks_cache_file}")
                                        with open(chunks_cache_file, 'wb') as f: pickle.dump(chunks, f)
                                        logger.info(f"Saving {len(embeddings)} embeddings to: {embeddings_cache_file}")
                                        np.save(embeddings_cache_file, np.array(embeddings, dtype=object), allow_pickle=True)
                                        source_metadata = get_source_files_metadata(documents_dir)
                                        with open(cache_metadata_file, 'w') as f: json.dump(source_metadata, f, indent=4)
                                        logger.info("Cache saved successfully.")
                                    except Exception as e:
                                        logger.error(f"Error saving to cache: {e}", exc_info=True)
                                
                                def load_from_cache(chunks_cache_file: Path, embeddings_cache_file: Path) -> Tuple[Optional[List[Document]], Optional[List[List[float]]]]:
                                    try:
                                        logger.info(f"Loading chunks from: {chunks_cache_file}")
                                        with open(chunks_cache_file, 'rb') as f: chunks = pickle.load(f)
                                        logger.info(f"Loading embeddings from: {embeddings_cache_file}")
                                        embeddings_array = np.load(embeddings_cache_file, allow_pickle=True)
                                        embeddings = [list(emb) for emb in embeddings_array]
                                        if len(chunks) != len(embeddings): return None, None
                                        logger.info(f"Loaded {len(chunks)} chunks and {len(embeddings)} embeddings from cache.")
                                        return chunks, embeddings
                                    except Exception:
                                        return None, None
                                
                                def _check_link_with_llm(article_summary: str, paradigm_description: str) -> bool:
                                    prompt_template = PromptTemplate.from_template(
                                        """You are a research assistant. Based on the article summary and the paradigm description below, does the article concretely investigate or provide a primary example of this experimental paradigm?
                                        Your answer MUST be a single word: YES or NO.
                                        Article Summary:
                                        {summary_text}
                                        ---
                                        Paradigm Description:
                                        {paradigm_description}
                                        Answer (YES or NO):"""
                                    )
                                    llm = llm_generator.get_llm(model_name=GEMINI_FLASH_MODEL_NAME, temperature=0.0)
                                    if not llm: return False
                                    chain = prompt_template | llm
                                    try:
                                        response = chain.invoke({"summary_text": article_summary, "paradigm_description": paradigm_description})
                                        return "YES" in response.content.strip().upper()
                                    except Exception as e:
                                        logger.error(f"LLM call failed during link check: {e}", exc_info=True)
                                        time.sleep(2)
                                        return False
                                
                                def run_full_ingestion_pipeline(documents_dir: Optional[Path] = None, clear_vector_store: bool = True, force_reprocess: bool = False):
                                    logger.info("--- Starting FINAL Corrected Ingestion & Linking Pipeline ---")
                                    if documents_dir is None: documents_dir = RAW_DOCUMENTS_PATH
                                
                                    manual_links = {}
                                    manual_links_path = DATA_DIR / "manual_links.json"
                                    if manual_links_path.exists():
                                        try:
                                            with open(manual_links_path, 'r') as f:
                                                manual_links = json.load(f)
                                            logger.info(f"Successfully loaded {len(manual_links)} articles from manual_links.json")
                                        except Exception as e:
                                            logger.error(f"Could not load or parse manual_links.json: {e}")
                                    
                                    if not force_reprocess and is_cache_valid(documents_dir, CACHE_METADATA_FILE, PREPARED_CHUNKS_CACHE_FILE, EMBEDDINGS_CACHE_FILE):
                                        logger.info("Valid cache found. Loading pre-processed and linked data from cache.")
                                        chunks, embeddings = load_from_cache(PREPARED_CHUNKS_CACHE_FILE, EMBEDDINGS_CACHE_FILE)
                                    else:
                                        if force_reprocess: logger.info("Force reprocessing. Bypassing cache.")
                                        
                                        all_elements = doc_loader.load_all_documents(documents_dir)
                                        
                                        handbook_path_name = "BEST_Handbook of CNfA Experimental Paradigms.docx"
                                        handbook_elements = [e for e in all_elements if e.metadata.get("file_name") == handbook_path_name]
                                        
                                        if not handbook_elements:
                                            logger.error(f"CRITICAL: Handbook ('{handbook_path_name}') not found. Cannot perform linking.")
                                            return
                                
                                        logger.info("--- Step 1a: Processing Handbook ---")
                                        handbook_chunks = doc_preprocessor.chunk_all_document_elements(handbook_elements)
                                        paradigm_defs = {chunk.metadata.get("paradigm_name"): chunk.page_content for chunk in handbook_chunks if chunk.metadata.get("paradigm_section") == "What is Studied" and "Uncategorized" not in chunk.metadata.get("paradigm_name", "") and "General Introduction" not in chunk.metadata.get("paradigm_name", "")}
                                        logger.info(f"Extracted {len(paradigm_defs)} paradigm definitions from the handbook.")
                                        
                                        all_final_chunks = list(handbook_chunks)
                                        
                                        article_elements_grouped = {}
                                        for element in all_elements:
                                            file_name = element.metadata.get("file_name")
                                            if file_name and file_name != handbook_path_name:
                                                article_elements_grouped.setdefault(file_name, []).append(element)
                                
                                        for file_name, elements in article_elements_grouped.items():
                                            logger.info(f"\n--- Step 1b: Processing Article: {file_name} ---")
                                            article_chunks = doc_preprocessor.chunk_all_document_elements(elements)
                                            
                                            found_links = []
                                            if file_name in manual_links:
                                                found_links = manual_links.get(file_name, [])
                                                logger.info(f"MANUAL LINK FOUND for '{file_name}': {found_links}")
                                            elif paradigm_defs:
                                                summary = generate_and_cache_article_summary(file_name=file_name)
                                                if summary:
                                                    for name, desc in paradigm_defs.items():
                                                        if _check_link_with_llm(summary, desc):
                                                            logger.info(f"AUTO LINK FOUND: '{file_name}' -> '{name}'")
                                                            found_links.append(name)
                                                else:
                                                    logger.warning(f"Could not generate summary for {file_name}, skipping auto-linking.")
                                            
                                            if found_links:
                                                logger.info(f"Attaching {len(found_links)} links to all chunks of {file_name}")
                                                for chunk in article_chunks:
                                                    chunk.metadata['linked_paradigm'] = json.dumps(found_links)
                                            
                                            all_final_chunks.extend(article_chunks)
                                
                                        logger.info(f"\n--- Step 2: Embedding all {len(all_final_chunks)} processed chunks ---")
                                        chunks, embeddings = doc_embedder.embed_prepared_documents(all_final_chunks)
                                        if not chunks:
                                            logger.error("Halted: Embedding process failed."); return
                                            
                                        save_to_cache(chunks, embeddings, documents_dir, CACHE_METADATA_FILE, PREPARED_CHUNKS_CACHE_FILE, EMBEDDINGS_CACHE_FILE)
                                
                                    if not chunks or not embeddings:
                                        logger.error("Halted: No chunks or embeddings available to store."); return
                                
                                    logger.info(f"--- Step 3: Storing {len(chunks)} final chunks in the vector store ---")
                                    if vs_manager.add_documents_to_store(documents=chunks, embeddings=embeddings, clear_existing_collection=clear_vector_store):
                                        logger.info("--- Ingestion & Linking Pipeline Complete ---")
                                    else:
                                        logger.error("--- Ingestion & Linking Pipeline Failed ---")
                                
                                _qa_pipeline_cache: Dict[Tuple, RetrievalQA] = {}
                                def setup_qa_chain(user_query: Optional[str] = None, k_articles_for_selection: int = 3, article_filter_categories: List[str] = ["Title", "Abstract", "Introduction", "Header", "NarrativeText"], retriever_k_results: int = RETRIEVER_K_RESULTS, retriever_search_type: str = RETRIEVER_SEARCH_TYPE, llm_model_name: str = GEMINI_FLASH_MODEL_NAME, llm_temperature: float = LLM_TEMPERATURE_QA, prompt: BasePromptTemplate = llm_prompts.QA_PROMPT, chain_type: str = "stuff") -> Optional[RetrievalQA]:
                                    # ... (this function is unchanged)
                                    logger.info("Setting up QA chain...")
                                    target_filenames: Optional[List[str]] = None
                                    if user_query:
                                        logger.info("Two-step retrieval: Selecting relevant articles.")
                                        target_filenames = vs_manager.get_relevant_article_filenames(query=user_query, k_articles=k_articles_for_selection, filter_categories=article_filter_categories)
                                        if not target_filenames: logger.warning("Two-step: No specific articles found.")
                                    cache_key_retriever_part = tuple(sorted(target_filenames)) if target_filenames else ("global",)
                                    cache_key_full = (cache_key_retriever_part, retriever_k_results, retriever_search_type, llm_model_name, llm_temperature, prompt.template)
                                    if cache_key_full in _qa_pipeline_cache:
                                        return _qa_pipeline_cache[cache_key_full]
                                    try:
                                        retriever = vs_manager.get_retriever(k_results=retriever_k_results, search_type=search_type, target_filenames=target_filenames)
                                        if retriever is None: return None
                                        llm = llm_generator.get_llm(model_name=llm_model_name, temperature=llm_temperature)
                                        if llm is None: return None
                                        qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type=chain_type, retriever=retriever, return_source_documents=True, chain_type_kwargs={"prompt": prompt})
                                        logger.info(f"RetrievalQA pipeline configured: targeting: {target_filenames if target_filenames else 'all docs'}.")
                                        _qa_pipeline_cache[cache_key_full] = qa_chain
                                        return qa_chain
                                    except Exception as e:
                                        logger.error(f"Error setting up QA pipeline: {e}", exc_info=True)
                                        return None
                                
                                def generate_and_cache_article_summary(file_name: str, documents_dir: Path = RAW_DOCUMENTS_PATH, summaries_dir: Path = PROCESSED_DATA_PATH / "summaries") -> Optional[str]:
                                    # ... (this function is unchanged)
                                    if not file_name: return None
                                    file_path = documents_dir / file_name
                                    summaries_dir.mkdir(parents=True, exist_ok=True)
                                    summary_cache_file = summaries_dir / f"{Path(file_name).stem}_3pager_summary.md"
                                    if summary_cache_file.exists():
                                        logger.info(f"Loading cached summary for '{file_name}' from: {summary_cache_file}")
                                        try:
                                            with open(summary_cache_file, 'r', encoding='utf-8') as f: return f.read()
                                        except Exception as e:
                                            logger.error(f"Error loading cached summary: {e}. Regenerating.", exc_info=True)
                                    if not file_path.is_file():
                                        logger.error(f"Article file not found: {file_path}")
                                        return None
                                    logger.info(f"No cached summary found for '{file_name}'. Generating new summary...")
                                    article_full_text = doc_loader.get_full_text_for_article(file_path)
                                    if not article_full_text: return None
                                    summary_text = llm_generator.generate_detailed_article_summary(article_full_text)
                                    if summary_text:
                                        try:
                                            with open(summary_cache_file, 'w', encoding='utf-8') as f: f.write(summary_text)
                                            logger.info(f"Saved new summary for '{file_name}' to: {summary_cache_file}")
                                        except Exception as e:
                                            logger.error(f"Error saving generated summary: {e}", exc_info=True)
                                        return summary_text
                                    return None
--------------------
            embedding/
                __init__.py:
                                
--------------------
                embedder.py:
                                import time
                                import random
                                from typing import List, Optional, Any # Ensure Any is imported
                                
                                from langchain_core.documents import Document
                                from langchain_core.embeddings import Embeddings # Import base class
                                import google.generativeai as genai
                                
                                # Import from our project
                                try:
                                    from ..config import GOOGLE_API_KEY, EMBEDDING_MODEL_NAME
                                    from ..config import EMBEDDING_TASK_TYPE_DOCUMENT, EMBEDDING_TASK_TYPE_QUERY # Ensure both are in config
                                    from ..utils.logging_config import logger
                                except ImportError:
                                    import sys
                                    from pathlib import Path
                                    SRC_DIR = Path(__file__).resolve().parent.parent.parent
                                    if str(SRC_DIR) not in sys.path:
                                        sys.path.insert(0, str(SRC_DIR))
                                    from cna_rag_agent.config import GOOGLE_API_KEY, EMBEDDING_MODEL_NAME
                                    from cna_rag_agent.config import EMBEDDING_TASK_TYPE_DOCUMENT, EMBEDDING_TASK_TYPE_QUERY
                                    from cna_rag_agent.utils.logging_config import logger
                                
                                if GOOGLE_API_KEY:
                                    try:
                                        genai.configure(api_key=GOOGLE_API_KEY)
                                        logger.info("Google Generative AI client (for direct calls) configured successfully in embedder.")
                                    except Exception as e:
                                        logger.error(f"Failed to configure Google Generative AI client (for direct calls) in embedder: {e}", exc_info=True)
                                else:
                                    logger.error("CRITICAL in embedder: GOOGLE_API_KEY not found. Embedding will fail.")
                                
                                
                                def get_embedding_with_backoff(
                                    text: str,
                                    model_name: str = EMBEDDING_MODEL_NAME,
                                    task_type_str: str = EMBEDDING_TASK_TYPE_DOCUMENT,
                                    max_retries: int = 7,
                                    base_delay_seconds: float = 4.0
                                ) -> Optional[List[float]]:
                                    if not GOOGLE_API_KEY:
                                        logger.error("Cannot generate embedding: GOOGLE_API_KEY is not configured.")
                                        return None
                                    if not text or not text.strip():
                                        logger.debug("Empty or whitespace-only text provided to get_embedding_with_backoff, returning None.")
                                        return None
                                    effective_task_type = task_type_str.upper()
                                    for attempt in range(max_retries):
                                        try:
                                            result = genai.embed_content(
                                                model=model_name, content=text, task_type=effective_task_type
                                            )
                                            logger.debug(f"API call for text '{text[:50]}...' returned type: {type(result)}")
                                            if isinstance(result, dict):
                                                embedding_list = result.get('embedding')
                                                if isinstance(embedding_list, list): return embedding_list
                                                else: raise Exception(f"genai.embed_content returned a dict, but 'embedding' key was not a list or was None. Value: {embedding_list}")
                                            elif hasattr(result, 'embedding') and result.embedding and \
                                                 hasattr(result.embedding, 'values') and isinstance(result.embedding.values, list):
                                                return result.embedding.values
                                            else: raise Exception(f"Unexpected API response structure from genai.embed_content. Type: {type(result)}, Result: {str(result)[:200]}")
                                        except Exception as e:
                                            error_type_name = type(e).__name__; error_message_snippet = str(e)[:200]
                                            if attempt < max_retries - 1:
                                                wait_time = base_delay_seconds * (2 ** attempt) + random.uniform(0.1, 1.0)
                                                logger.warning(f"Embedding attempt {attempt + 1}/{max_retries} failed for text '{text[:60]}...': {error_type_name} - {error_message_snippet}. Retrying in {wait_time:.2f} seconds...")
                                                time.sleep(wait_time)
                                            else: logger.error(f"Embedding attempt {attempt + 1}/{max_retries} (final) failed for text '{text[:60]}...': {error_type_name} - {error_message_snippet}.")
                                    logger.error(f"All {max_retries} retries failed for text: '{text[:60]}...'"); return None
                                
                                class CustomGeminiEmbeddings(Embeddings):
                                    """
                                    Custom Langchain Embeddings class that uses our robust get_embedding_with_backoff
                                    for embedding documents and queries via direct genai.embed_content calls.
                                    """
                                    # These fields will be initialized by Pydantic from class variables/defaults
                                    # or can be overridden if an __init__ that accepts them is defined and called.
                                    # Since they use config values directly, no __init__ is strictly needed if no other logic is required at init.
                                    model_name: str = EMBEDDING_MODEL_NAME
                                    task_type_document: str = EMBEDDING_TASK_TYPE_DOCUMENT
                                    task_type_query: str = EMBEDDING_TASK_TYPE_QUERY
                                    # Add any other Pydantic fields if needed, e.g., client, api_key, etc.
                                    # However, for this setup, get_embedding_with_backoff is self-contained.
                                
                                    def embed_documents(self, texts: List[str]) -> List[List[float]]:
                                        logger.info(f"CustomGeminiEmbeddings: Embedding {len(texts)} documents using model '{self.model_name}'.")
                                        embeddings_list: List[List[float]] = [] # Ensure it's initialized for the case of all failures
                                        
                                        # Using tqdm for progress if many documents (optional, can be removed if not desired here)
                                        # from tqdm import tqdm
                                        # for text_item in tqdm(texts, desc="Embedding documents (custom)", unit="doc"):
                                        for text_item in texts:
                                            emb = get_embedding_with_backoff(
                                                text=text_item,
                                                model_name=self.model_name,
                                                task_type_str=self.task_type_document # Use the instance's task_type_document
                                            )
                                            if emb is None:
                                                logger.warning(f"Failed to embed document text: '{text_item[:70]}...'. Using zero vector.")
                                                embeddings_list.append([0.0] * 768) # Default to zero vector of correct dimensionality
                                            else:
                                                embeddings_list.append(emb)
                                        return embeddings_list
                                
                                    def embed_query(self, text: str) -> List[float]:
                                        logger.info(f"CustomGeminiEmbeddings: Embedding query '{text[:70]}...' using model '{self.model_name}'.")
                                        emb = get_embedding_with_backoff(
                                            text=text,
                                            model_name=self.model_name,
                                            task_type_str=self.task_type_query # Use the instance's task_type_query
                                        )
                                        if emb is None:
                                            logger.error(f"Failed to embed query: '{text[:70]}...'. Returning zero vector.")
                                            return [0.0] * 768 # Default to zero vector of correct dimensionality
                                        return emb
                                
                                _custom_embedding_model_instance: Optional[CustomGeminiEmbeddings] = None
                                
                                def get_embedding_model() -> CustomGeminiEmbeddings: # Return our custom class
                                    """
                                    Initializes and returns an instance of our CustomGeminiEmbeddings class.
                                    Relies on class variables for configuration.
                                    """
                                    global _custom_embedding_model_instance
                                    if _custom_embedding_model_instance is None:
                                        if not GOOGLE_API_KEY:
                                            logger.error("GOOGLE_API_KEY not found. Cannot initialize custom embedding model.")
                                            raise ValueError("GOOGLE_API_KEY is not set for custom embedding model.")
                                        try:
                                            logger.info(f"Initializing CustomGeminiEmbeddings (will use class defaults from config: model={EMBEDDING_MODEL_NAME})")
                                            # Instantiate WITHOUT passing arguments; Pydantic will use class variable defaults
                                            _custom_embedding_model_instance = CustomGeminiEmbeddings()
                                            logger.info("CustomGeminiEmbeddings model initialized successfully.")
                                        except Exception as e:
                                            logger.error(f"Failed to initialize CustomGeminiEmbeddings: {e}", exc_info=True)
                                            raise
                                    return _custom_embedding_model_instance
                                
                                
                                def embed_prepared_documents(prepared_chunks: List[Document]) -> tuple[List[Document], Optional[List[List[float]]]]:
                                    if not prepared_chunks: logger.warning("No prepared chunks provided for embedding."); return [], None
                                    if not GOOGLE_API_KEY: logger.error("Cannot embed documents: GOOGLE_API_KEY is not configured."); return prepared_chunks, None
                                    logger.info(f"Starting embedding process for {len(prepared_chunks)} prepared chunks using custom backoff (direct genai call).")
                                    successfully_embedded_chunks: List[Document] = []
                                    corresponding_embedding_vectors: List[List[float]] = []
                                    from tqdm import tqdm
                                    for i, chunk in enumerate(tqdm(prepared_chunks, desc="Embedding Chunks", unit="chunk")):
                                        embedding_vector = get_embedding_with_backoff(
                                            text=chunk.page_content, model_name=EMBEDDING_MODEL_NAME, task_type_str=EMBEDDING_TASK_TYPE_DOCUMENT
                                        )
                                        if embedding_vector is not None:
                                            successfully_embedded_chunks.append(chunk); corresponding_embedding_vectors.append(embedding_vector)
                                        else: logger.warning(f"Failed to embed chunk {i+1}/{len(prepared_chunks)} (source: {chunk.metadata.get('file_name', 'N/A')}, content: '{chunk.page_content[:50]}...') Will be excluded.")
                                    failed_count = len(prepared_chunks) - len(successfully_embedded_chunks)
                                    logger.info(f"Embedding process completed. Successfully embedded: {len(successfully_embedded_chunks)}/{len(prepared_chunks)} chunks.")
                                    if failed_count > 0: logger.warning(f"{failed_count} chunks could not be embedded after all retries and were excluded.")
                                    if not successfully_embedded_chunks: logger.error("Embedding process resulted in no successfully embedded chunks."); return [], None
                                    return successfully_embedded_chunks, corresponding_embedding_vectors
                                
                                
                                if __name__ == "__main__":
                                    logger.info("Running embedder.py directly for testing...")
                                    if not GOOGLE_API_KEY:
                                        logger.error("CRITICAL: GOOGLE_API_KEY not found. Please set it in .env for this test.")
                                    else:
                                        try:
                                            custom_embed_model = get_embedding_model()
                                            logger.info("Testing CustomGeminiEmbeddings.embed_query()...")
                                            query_embedding = custom_embed_model.embed_query("What is the future of AI research?")
                                            if query_embedding and len(query_embedding) == 768:
                                                logger.info(f"CustomGeminiEmbeddings.embed_query() test successful. Dim: {len(query_embedding)}. First 3: {query_embedding[:3]}")
                                            else:
                                                logger.error(f"CustomGeminiEmbeddings.embed_query() test failed or returned unexpected result. Embedding: {query_embedding}")
                                
                                            logger.info("Testing CustomGeminiEmbeddings.embed_documents()...")
                                            doc_texts_to_embed = ["Test document 1 for custom embedder.", "Another test document for the same."]
                                            doc_embeddings = custom_embed_model.embed_documents(doc_texts_to_embed)
                                            if doc_embeddings and len(doc_embeddings) == len(doc_texts_to_embed) and \
                                               (len(doc_embeddings[0]) == 768 if doc_embeddings[0] else False) :
                                                logger.info(f"CustomGeminiEmbeddings.embed_documents() test successful for {len(doc_embeddings)} docs.")
                                            else:
                                                logger.error(f"CustomGeminiEmbeddings.embed_documents() test failed or returned unexpected result. Embeddings: {doc_embeddings}")
                                        except Exception as e:
                                            logger.error(f"Error testing CustomGeminiEmbeddings: {e}", exc_info=True)
                                    logger.info("Embedder.py direct test finished.")
--------------------
            utils/
                logging_config.py:
                                # src/cna_rag_agent/utils/logging_config.py
                                import logging
                                import sys
                                from pathlib import Path # Added for type hinting if needed, not strictly necessary here
                                
                                # Attempt to import config values. This creates a slight circular dependency risk
                                # if this module is imported before config can resolve its own paths.
                                # A common pattern is to pass config values to this setup function or have config call it.
                                # For simplicity now, direct import, assuming config.py is loaded early.
                                try:
                                    from ..config import LOG_LEVEL, LOG_FILE_PATH
                                except ImportError:
                                    # Fallback if this module is somehow loaded in a context where relative import fails
                                    # or config isn't set up (e.g., during a unit test of this module itself).
                                    print("WARNING: Could not import LOG_LEVEL, LOG_FILE_PATH from config. Using defaults for logging.")
                                    LOG_LEVEL = "INFO"
                                    LOG_FILE_PATH = Path(".") / "temp_app.log" # Temporary fallback
                                
                                _logger_initialized = False
                                
                                def setup_logging(log_level_str: str = LOG_LEVEL, log_file: Path = LOG_FILE_PATH) -> logging.Logger:
                                    """
                                    Configures and returns a logger for the application.
                                    Avoids adding multiple handlers if called multiple times.
                                    """
                                    global _logger_initialized
                                    logger = logging.getLogger("cna_rag_agent") # Get a specific logger for our app
                                
                                    if _logger_initialized and logger.hasHandlers():
                                         # Logger already configured by a previous call in this session
                                        return logger
                                
                                    numeric_level = getattr(logging, log_level_str.upper(), None)
                                    if not isinstance(numeric_level, int):
                                        # Fallback to INFO if an invalid level string is provided
                                        print(f"WARNING: Invalid log level '{log_level_str}'. Defaulting to INFO.")
                                        numeric_level = logging.INFO
                                
                                    logger.setLevel(numeric_level)
                                
                                    # Console Handler
                                    console_handler = logging.StreamHandler(sys.stdout)
                                    console_handler.setLevel(numeric_level) # Set level for handler too
                                
                                    # File Handler
                                    # Ensure the directory for the log file exists
                                    log_file.parent.mkdir(parents=True, exist_ok=True)
                                    file_handler = logging.FileHandler(log_file, mode='a') # Append mode
                                    file_handler.setLevel(numeric_level) # Set level for handler too
                                
                                    # Formatter
                                    formatter = logging.Formatter(
                                        "%(asctime)s - %(name)s - %(levelname)s - [%(module)s.%(funcName)s:%(lineno)d] - %(message)s"
                                    )
                                    console_handler.setFormatter(formatter)
                                    file_handler.setFormatter(formatter)
                                
                                    # Add handlers to the logger
                                    logger.addHandler(console_handler)
                                    logger.addHandler(file_handler)
                                
                                    logger.propagate = False # Prevent root logger from also handling these messages if it's configured
                                    _logger_initialized = True
                                
                                    # Test message to confirm setup
                                    # logger.info(f"Logging initialized. Level: {log_level_str}. File: {log_file}")
                                    return logger
                                
                                # Initialize and expose the logger for other modules to import
                                logger = setup_logging()
--------------------
                __init__.py:
                                
--------------------
            agent/
                tools.py:
                                # src/cna_rag_agent/agent/tools.py
                                
                                import sys
                                import json
                                from pathlib import Path
                                
                                # --- Setup Project Path ---
                                SRC_DIR = Path(__file__).resolve().parent.parent.parent
                                if str(SRC_DIR) not in sys.path:
                                    sys.path.insert(0, str(SRC_DIR))
                                # --- End Setup Project Path ---
                                
                                from langchain_core.documents import Document
                                from langchain_core.output_parsers import StrOutputParser
                                from langchain_core.prompts import PromptTemplate
                                
                                from cna_rag_agent.utils.logging_config import logger
                                from cna_rag_agent.vector_store.store_manager import get_vector_store
                                from cna_rag_agent.generation.generator import get_llm
                                from cna_rag_agent.pipeline.rag_pipeline import generate_and_cache_article_summary, setup_qa_chain
                                from cna_rag_agent.config import GEMINI_FLASH_MODEL_NAME, GEMINI_PRO_MODEL_NAME
                                
                                def get_introduction_for_topic(topic_name: str) -> str:
                                    # This tool is confirmed to be working correctly.
                                    logger.info(f"Tool 'get_introduction_for_topic' called for topic: '{topic_name}'")
                                    vector_store = get_vector_store()
                                    if not vector_store:
                                        return "Error: Could not access the vector store."
                                
                                    search_term = topic_name.split('. ', 1)[-1].strip()
                                    collection = vector_store._collection
                                    all_handbook_results = collection.get(
                                        where={"content_type": "handbook_entry"},
                                        include=["documents", "metadatas"]
                                    )
                                    
                                    final_docs = []
                                    if all_handbook_results and all_handbook_results['documents']:
                                        for doc_content, meta in zip(all_handbook_results['documents'], all_handbook_results['metadatas']):
                                            category = meta.get("category", "")
                                            section = meta.get("paradigm_section", "")
                                            if search_term in category and section in ["What is Studied", "Why it Matters"]:
                                                final_docs.append(Document(page_content=doc_content, metadata=meta))
                                
                                    if not final_docs:
                                        return f"Sorry, I could not find any introductory content for the topic: {topic_name}"
                                
                                    context = "\n\n".join(doc.page_content for doc in final_docs)
                                    prompt = PromptTemplate.from_template(
                                        """You are a helpful research assistant. Based ONLY on the following context from the CNfA Handbook, write a concise and clear introduction to the topic. Combine the 'What is Studied' and 'Why it Matters' sections into a fluid, well-written paragraph.
                                Context:
                                {context}
                                Introduction:"""
                                    )
                                    llm = get_llm(model_name=GEMINI_FLASH_MODEL_NAME, temperature=0.1)
                                    rag_chain = ({ "context": lambda x: context } | prompt | llm | StrOutputParser())
                                    result = rag_chain.invoke("") 
                                    logger.info(f"Successfully generated introduction for topic: '{topic_name}'")
                                    return result
                                
                                def list_articles_and_summaries_for_topic(topic_name: str) -> str:
                                    # This tool is confirmed to be working correctly.
                                    logger.info(f"Tool 'list_articles_and_summaries_for_topic' called for topic: '{topic_name}'")
                                    vector_store = get_vector_store()
                                    if not vector_store:
                                        return "Error: Could not access the vector store."
                                    
                                    collection = vector_store._collection
                                    all_docs_results = collection.get(include=["metadatas"])
                                    
                                    if not all_docs_results or not all_docs_results['ids']:
                                        return "Could not retrieve any documents from the database."
                                        
                                    simple_search_term = topic_name.split('→')[0].strip().split('. ')[-1]
                                
                                    linked_files = set()
                                    for meta in all_docs_results['metadatas']:
                                        linked_paradigm_str = meta.get('linked_paradigm')
                                        if linked_paradigm_str and simple_search_term in linked_paradigm_str:
                                            if 'file_name' in meta:
                                                linked_files.add(meta['file_name'])
                                
                                    if not linked_files:
                                        return f"I could not find any articles specifically linked to the topic: '{topic_name}'."
                                
                                    output = []
                                    output.append(f"Found {len(linked_files)} article(s) linked to '{topic_name}':\n")
                                    
                                    for file_name in linked_files:
                                        output.append(f"\n{'='*20}\nARTICLE: {file_name}\n{'='*20}\n")
                                        summary = generate_and_cache_article_summary(file_name=file_name)
                                        if summary:
                                            output.append(summary)
                                        else:
                                            output.append(f"Could not generate or find a summary for {file_name}.")
                                        output.append("\n\n")
                                
                                    return "".join(output)
                                
                                def deep_dive_into_methods(paradigm_name: str) -> str:
                                    # This tool is confirmed to be working correctly.
                                    logger.info(f"Tool 'deep_dive_into_methods' called for paradigm: '{paradigm_name}'")
                                    vector_store = get_vector_store()
                                    if not vector_store:
                                        return "Error: Could not access the vector store."
                                        
                                    collection = vector_store._collection
                                    handbook_results = collection.get(
                                        where={"$and": [{"content_type": "handbook_entry"}, {"paradigm_name": paradigm_name}]},
                                        include=["documents", "metadatas"]
                                    )
                                    
                                    handbook_context = ""
                                    if handbook_results and handbook_results['documents']:
                                        sections = {}
                                        for doc, meta in zip(handbook_results['documents'], handbook_results['metadatas']):
                                            section_name = meta.get('paradigm_section', 'Details')
                                            sections.setdefault(section_name, []).append(doc)
                                        
                                        for section_name, contents in sorted(sections.items()):
                                            handbook_context += f"### From Handbook: {section_name}\n"
                                            handbook_context += "\n".join(contents)
                                            handbook_context += "\n\n"
                                    
                                    article_summaries = list_articles_and_summaries_for_topic(paradigm_name)
                                    
                                    prompt = PromptTemplate.from_template(
                                        """You are an expert in Cognitive Neuroscience for Architecture. Synthesize a detailed "deep dive" on the methods used in the '{paradigm_name}' paradigm. Use the provided context from the official handbook and the summaries of related research articles. Structure your answer clearly.
                                **Handbook Context:**
                                {handbook_context}
                                ---
                                **Related Article Summaries:**
                                {article_summaries}
                                ---
                                **Synthesized Deep Dive Report:**
                                """
                                    )
                                    
                                    llm = get_llm(model_name=GEMINI_PRO_MODEL_NAME, temperature=0.1)
                                    chain = prompt | llm | StrOutputParser()
                                    
                                    result = chain.invoke({
                                        "paradigm_name": paradigm_name,
                                        "handbook_context": handbook_context or "No specific handbook details found.",
                                        "article_summaries": article_summaries if "I could not find any articles" not in article_summaries else "No linked articles found for this paradigm."
                                    })
                                    
                                    logger.info(f"Successfully generated deep dive for paradigm: '{paradigm_name}'")
                                    return result
                                
                                def identify_open_questions(topic_name: str) -> str:
                                    # This tool is confirmed to be working correctly.
                                    logger.info(f"Tool 'identify_open_questions' called for topic: '{topic_name}'")
                                    
                                    intro_context = get_introduction_for_topic(topic_name)
                                    article_summaries = list_articles_and_summaries_for_topic(topic_name)
                                
                                    prompt = PromptTemplate.from_template(
                                        """You are a scientific analyst. Based on the theoretical background of the topic and the limitations and future work mentioned in related research articles, identify and synthesize the key "open questions" and future research directions for the topic of '{topic_name}'.
                                Look for gaps between the theory and the findings. Synthesize explicitly mentioned limitations and suggestions for future studies into a coherent report.
                                **Theoretical Background (from Handbook):**
                                {handbook_context}
                                ---
                                **Related Article Summaries (Discussion, Limitations, Future Work):**
                                {article_summaries}
                                ---
                                **Synthesized Report on Open Questions & Future Directions:**
                                """
                                    )
                                
                                    llm = get_llm(model_name=GEMINI_PRO_MODEL_NAME, temperature=0.3)
                                    chain = prompt | llm | StrOutputParser()
                                
                                    result = chain.invoke({
                                        "topic_name": topic_name,
                                        "handbook_context": intro_context or "No handbook context available.",
                                        "article_summaries": article_summaries if "I could not find any articles" not in article_summaries else "No linked articles found to analyze for limitations or future work."
                                    })
                                    
                                    logger.info(f"Successfully identified open questions for topic: '{topic_name}'")
                                    return result
                                
                                def general_purpose_qa(question: str) -> str:
                                    # This tool is confirmed to be working correctly.
                                    logger.info(f"Tool 'general_purpose_qa' called with question: '{question}'")
                                    
                                    qa_chain = setup_qa_chain(user_query=question)
                                    
                                    if not qa_chain:
                                        return "Sorry, I was unable to set up the question-answering pipeline."
                                        
                                    try:
                                        response = qa_chain.invoke({"query": question})
                                        answer = response.get("result", "No answer found.")
                                        sources = response.get("source_documents", [])
                                        
                                        if sources:
                                            source_names = set(doc.metadata.get('file_name', 'Unknown') for doc in sources)
                                            answer += f"\n\nSources: {', '.join(source_names)}"
                                            
                                        return answer
                                    except Exception as e:
                                        logger.error(f"Error during general purpose Q&A: {e}", exc_info=True)
                                        return "An error occurred while trying to answer the question."
                                
                                # --- UPDATED TEST BLOCK WITH ROBUST ERROR HANDLING ---
                                if __name__ == "__main__":
                                    try:
                                        logger.info("--- Testing Agent Tools ---")
                                        
                                        intro_test_topic = "I. Spatial Interaction & Navigation Paradigms"
                                        print(f"\nRUNNING TEST 1: Get Introduction for '{intro_test_topic}'...")
                                        introduction = get_introduction_for_topic(intro_test_topic)
                                        print(introduction)
                                
                                        list_test_topic = "A. Environmental Priming → Creative Task"
                                        print(f"\nRUNNING TEST 2: List Articles for '{list_test_topic}'...")
                                        article_list = list_articles_and_summaries_for_topic(list_test_topic)
                                        print(article_list)
                                
                                        deep_dive_topic = "F. Landmark Salience → Wayfinding Performance" 
                                        print(f"\nRUNNING TEST 3: Deep Dive for '{deep_dive_topic}'...")
                                        method_details = deep_dive_into_methods(deep_dive_topic)
                                        print(method_details)
                                
                                        open_questions_topic = "I. Spatial Interaction & Navigation Paradigms"
                                        print(f"\nRUNNING TEST 4: Identify Open Questions for '{open_questions_topic}'...")
                                        open_questions_report = identify_open_questions(open_questions_topic)
                                        print(open_questions_report)
                                
                                        qa_question = "What is the theory of enactive perception?"
                                        print(f"\nRUNNING TEST 5: General Q&A for '{qa_question}'...")
                                        answer = general_purpose_qa(qa_question)
                                        print(answer)
                                        
                                    except Exception as e:
                                        logger.error("A critical error occurred in the main test block.", exc_info=True)
                                        print(f"\n\nA CRITICAL ERROR OCCURRED: {e}")
--------------------
            vector_store/
                __init__.py:
                                
--------------------
                store_manager.py:
                                # src/cna_rag_agent/vector_store/store_manager.py
                                
                                from typing import List, Optional, Any
                                from pathlib import Path # ENSURE THIS IS IMPORTED
                                
                                from langchain_core.documents import Document
                                from langchain_core.vectorstores import VectorStoreRetriever
                                from langchain_community.vectorstores import Chroma
                                import chromadb
                                
                                # Import from our project
                                try:
                                    from ..config import (
                                        CHROMA_PERSIST_DIR, CHROMA_COLLECTION_NAME, GOOGLE_API_KEY,
                                        RETRIEVER_K_RESULTS, RETRIEVER_SEARCH_TYPE,
                                        RETRIEVER_MMR_FETCH_K, RETRIEVER_MMR_LAMBDA_MULT
                                    )
                                    from ..utils.logging_config import logger
                                    from ..embedding.embedder import get_embedding_model
                                except ImportError:
                                    import sys
                                    # Path is already imported above, but if it were only here, it would be fine too for fallback.
                                    # from pathlib import Path # Not strictly needed again if imported at top level
                                    SRC_DIR = Path(__file__).resolve().parent.parent.parent
                                    if str(SRC_DIR) not in sys.path:
                                        sys.path.insert(0, str(SRC_DIR))
                                    from cna_rag_agent.config import (
                                        CHROMA_PERSIST_DIR, CHROMA_COLLECTION_NAME, GOOGLE_API_KEY,
                                        RETRIEVER_K_RESULTS, RETRIEVER_SEARCH_TYPE,
                                        RETRIEVER_MMR_FETCH_K, RETRIEVER_MMR_LAMBDA_MULT
                                    )
                                    from cna_rag_agent.utils.logging_config import logger
                                    from cna_rag_agent.embedding.embedder import get_embedding_model
                                
                                
                                _persistent_client: Optional[chromadb.PersistentClient] = None
                                
                                def get_persistent_client(persist_directory: Path = CHROMA_PERSIST_DIR) -> chromadb.PersistentClient:
                                    global _persistent_client
                                    if _persistent_client is None:
                                        logger.info(f"Initializing ChromaDB persistent client at: {persist_directory}")
                                        persist_directory.mkdir(parents=True, exist_ok=True)
                                        _persistent_client = chromadb.PersistentClient(path=str(persist_directory))
                                        logger.info("ChromaDB persistent client initialized.")
                                    return _persistent_client
                                
                                def get_vector_store(
                                    collection_name: str = CHROMA_COLLECTION_NAME,
                                    persist_directory: Path = CHROMA_PERSIST_DIR
                                ) -> Optional[Chroma]:
                                    try:
                                        client = get_persistent_client(persist_directory)
                                        embedding_function = get_embedding_model()
                                        vector_store = Chroma(
                                            client=client, collection_name=collection_name,
                                            embedding_function=embedding_function, persist_directory=str(persist_directory)
                                        )
                                        try:
                                            coll_obj = client.get_collection(name=collection_name)
                                            if coll_obj.count() == 0: logger.warning(f"Collection '{collection_name}' exists but is empty.")
                                            else: logger.info(f"Collection '{collection_name}' loaded with {coll_obj.count()} documents.")
                                        except Exception:
                                             logger.error(f"Collection '{collection_name}' does not exist in the database at {persist_directory}. Please run ingestion first.")
                                             return None
                                        return vector_store
                                    except Exception as e:
                                        logger.error(f"Failed to initialize Langchain Chroma vector store: {e}", exc_info=True)
                                        return None
                                
                                def get_relevant_article_filenames(
                                    query: str,
                                    k_articles: int = 3,
                                    filter_categories: List[str] = ["Title", "Abstract", "Introduction"],
                                    collection_name: str = CHROMA_COLLECTION_NAME,
                                    persist_directory: Path = CHROMA_PERSIST_DIR
                                ) -> List[str]:
                                    logger.info(f"Step 1 (Article Selection): Searching for relevant articles for query: '{query[:50]}...'")
                                    vector_store = get_vector_store(collection_name, persist_directory)
                                    if not vector_store:
                                        logger.error("Cannot get relevant article filenames: vector store not available.")
                                        return []
                                    search_filter = None
                                    if filter_categories:
                                        search_filter = {"category": {"$in": filter_categories}}
                                    logger.debug(f"Article selection using filter: {search_filter}")
                                    try:
                                        retrieved_elements = vector_store.similarity_search(query, k=k_articles * 5, filter=search_filter)
                                    except Exception as e:
                                        logger.error(f"Error during similarity search for article selection: {e}", exc_info=True); return []
                                    if not retrieved_elements: logger.info("No relevant title/abstract elements found for article selection."); return []
                                    relevant_filenames = []; seen_filenames = set()
                                    for doc in retrieved_elements:
                                        filename = doc.metadata.get("file_name")
                                        if filename and filename not in seen_filenames:
                                            relevant_filenames.append(filename); seen_filenames.add(filename)
                                            if len(relevant_filenames) >= k_articles: break
                                    logger.info(f"Step 1 (Article Selection): Found {len(relevant_filenames)} relevant article filenames: {relevant_filenames}")
                                    return relevant_filenames
                                
                                def get_retriever(
                                    k_results: int = RETRIEVER_K_RESULTS,
                                    search_type: str = RETRIEVER_SEARCH_TYPE,
                                    target_filenames: Optional[List[str]] = None
                                ) -> Optional[VectorStoreRetriever]:
                                    action = "global" if target_filenames is None else f"targeted ({len(target_filenames)} files)"
                                    logger.info(f"Attempting to create {action} retriever. Search type: '{search_type}', k: {k_results}")
                                    vector_store = get_vector_store()
                                    if vector_store:
                                        search_kwargs: Dict[str, Any] = {"k": k_results}
                                        if target_filenames:
                                            if not isinstance(target_filenames, list) or not all(isinstance(fn, str) for fn in target_filenames):
                                                logger.error("Invalid target_filenames provided; must be a list of strings."); return None
                                            search_kwargs["filter"] = {"file_name": {"$in": target_filenames}}
                                            logger.info(f"Retriever will be filtered for filenames: {target_filenames}")
                                        if search_type == "mmr":
                                            search_kwargs["fetch_k"] = RETRIEVER_MMR_FETCH_K
                                            search_kwargs["lambda_mult"] = RETRIEVER_MMR_LAMBDA_MULT
                                        try:
                                            retriever = vector_store.as_retriever(search_type=search_type, search_kwargs=search_kwargs)
                                            logger.info(f"Successfully created {action} retriever with search_type='{search_type}', k={k_results}.")
                                            return retriever
                                        except Exception as e: logger.error(f"Failed to create {action} retriever from vector store: {e}", exc_info=True); return None
                                    else: logger.error("Failed to get vector store, cannot create retriever."); return None
                                
                                def _sanitize_metadata_value(value: Any) -> Any:
                                    if isinstance(value, (str, int, float, bool)): return value
                                    try: return str(value)
                                    except Exception as e: logger.warning(f"Could not convert metadata value '{str(value)[:100]}' to string: {e}. Using empty string."); return ""
                                def _sanitize_metadata(metadata: dict) -> dict:
                                    if metadata is None: return {}
                                    return {key: _sanitize_metadata_value(value) for key, value in metadata.items()}
                                def add_documents_to_store(documents: List[Document], embeddings: List[List[float]], collection_name: str = CHROMA_COLLECTION_NAME, persist_directory: Path = CHROMA_PERSIST_DIR, clear_existing_collection: bool = False) -> bool:
                                    if not documents or not embeddings: logger.warning("No documents or embeddings provided. Skipping."); return False
                                    if len(documents) != len(embeddings): logger.error(f"Mismatch docs/embeddings. Cannot add."); return False
                                    logger.info(f"Adding {len(documents)} docs to '{collection_name}'. Clear: {clear_existing_collection}")
                                    try:
                                        client = get_persistent_client(persist_directory)
                                        if clear_existing_collection:
                                            try: logger.info(f"Deleting collection: '{collection_name}'"); client.delete_collection(name=collection_name); logger.info(f"Deleted collection: '{collection_name}'.")
                                            except ValueError: logger.info(f"Collection '{collection_name}' not found for deletion.")
                                            except Exception as e: logger.error(f"Error deleting collection '{collection_name}': {e}", exc_info=True)
                                        collection = client.get_or_create_collection(name=collection_name)
                                        doc_texts = [doc.page_content for doc in documents]
                                        doc_metadatas = [_sanitize_metadata(doc.metadata) for doc in documents]
                                        doc_ids = [f"chunk_{i}_{Path(doc.metadata.get('file_name', 'unknown')).stem}_pg{str(doc.metadata.get('page_number', 'na'))}_{str(abs(hash(doc.page_content)))[:10]}" for i, doc in enumerate(documents)]
                                        batch_size = 200; num_batches = (len(documents) + batch_size - 1) // batch_size
                                        for i in range(num_batches):
                                            start_idx = i * batch_size; end_idx = min((i + 1) * batch_size, len(documents))
                                            logger.info(f"Adding batch {i+1}/{num_batches} to Chroma '{collection_name}' ({len(doc_texts[start_idx:end_idx])} docs).")
                                            collection.add(embeddings=embeddings[start_idx:end_idx], documents=doc_texts[start_idx:end_idx], metadatas=doc_metadatas[start_idx:end_idx], ids=doc_ids[start_idx:end_idx])
                                        logger.info(f"Added/updated {len(documents)} docs in '{collection_name}'. Total in collection: {collection.count()}")
                                        return True
                                    except Exception as e: logger.error(f"Failed to add docs to Chroma '{collection_name}': {e}", exc_info=True); return False
                                
                                if __name__ == "__main__":
                                    logger.warning("Directly running store_manager.py. Test block attempts to use embeddings and query.")
                                    if GOOGLE_API_KEY:
                                        logger.info("Running store_manager.py direct test...")
                                        vs_instance = get_vector_store()
                                        if vs_instance and vs_instance._collection.count() > 0:
                                            logger.info(f"Vector store '{CHROMA_COLLECTION_NAME}' loaded with {vs_instance._collection.count()} documents.")
                                            test_query = "embodied cognition"
                                            logger.info(f"\nTesting get_relevant_article_filenames for query: '{test_query}'")
                                            article_filenames = get_relevant_article_filenames(query=test_query, k_articles=2)
                                            if article_filenames: logger.info(f"Found relevant article filenames: {article_filenames}")
                                            else: logger.warning("No relevant article filenames found for the test query.")
                                            logger.info(f"\nTesting global get_retriever for query: '{test_query}'")
                                            global_retriever = get_retriever(k_results=3)
                                            if global_retriever:
                                                retrieved_docs_global = global_retriever.invoke(test_query)
                                                logger.info(f"Global retriever found {len(retrieved_docs_global)} documents:")
                                                for i, doc_result in enumerate(retrieved_docs_global): logger.info(f"  G-Result {i+1}: '{doc_result.page_content[:70]}...' Metadata: {doc_result.metadata.get('file_name')}, {doc_result.metadata.get('category')}")
                                            else: logger.error("Failed to create global retriever.")
                                        else: logger.warning("Vector store is empty/not loaded. Run ingestion (`python src/main.py ingest`) first.")
                                    else: logger.error("Direct test skipped: GOOGLE_API_KEY not set.")
                                    logger.info("Store_manager.py direct test finished.")
--------------------
            retrieval/
                retriever.py:
                                
--------------------
                __init__.py:
                                
--------------------
            data_ingestion/
                __init__.py:
                                
--------------------
                preprocessor.py:
                                # src/cna_rag_agent/data_ingestion/preprocessor.py
                                
                                from typing import List
                                
                                from langchain_core.documents import Document
                                from langchain.text_splitter import RecursiveCharacterTextSplitter
                                import tiktoken
                                
                                # Primary attempt for package-based import
                                try:
                                    from ..config import CHUNK_SIZE_TOKENS, CHUNK_OVERLAP_TOKENS, TOKENIZER_MODEL_REFERENCE
                                    from ..utils.logging_config import logger
                                except ImportError:
                                    # Fallback for direct script execution
                                    import sys
                                    from pathlib import Path
                                    SRC_DIR = Path(__file__).resolve().parent.parent.parent # Should navigate up to 'src'
                                    if str(SRC_DIR) not in sys.path:
                                        sys.path.insert(0, str(SRC_DIR))
                                    from cna_rag_agent.config import CHUNK_SIZE_TOKENS, CHUNK_OVERLAP_TOKENS, TOKENIZER_MODEL_REFERENCE
                                    from cna_rag_agent.utils.logging_config import logger
                                
                                MIN_ELEMENT_LENGTH_FOR_CHUNKING = 10
                                
                                def get_tokenizer_for_reference_counting(encoding_name: str = "cl100k_base") -> tiktoken.Encoding:
                                    try:
                                        tokenizer = tiktoken.get_encoding(encoding_name)
                                    except KeyError:
                                        logger.error(f"Encoding '{encoding_name}' not found by tiktoken.get_encoding.")
                                        raise
                                    return tokenizer
                                
                                def chunk_all_document_elements(loaded_elements: List[Document]) -> List[Document]:
                                    logger.info(f"Starting chunking process for {len(loaded_elements)} loaded document elements.")
                                    logger.info(f"Using TOKENIZER_MODEL_REFERENCE: '{TOKENIZER_MODEL_REFERENCE}' for RecursiveCharacterTextSplitter.")
                                
                                    try:
                                        text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
                                            model_name=TOKENIZER_MODEL_REFERENCE,
                                            chunk_size=CHUNK_SIZE_TOKENS,
                                            chunk_overlap=CHUNK_OVERLAP_TOKENS,
                                        )
                                        logger.info(f"Successfully initialized RecursiveCharacterTextSplitter with tiktoken model: '{TOKENIZER_MODEL_REFERENCE}'.")
                                    except Exception as e:
                                        logger.error(f"Failed to initialize RecursiveCharacterTextSplitter with tiktoken model '{TOKENIZER_MODEL_REFERENCE}': {e}", exc_info=True)
                                        logger.info("Falling back to character-based RecursiveCharacterTextSplitter.")
                                        char_chunk_size = CHUNK_SIZE_TOKENS * 4
                                        char_chunk_overlap = CHUNK_OVERLAP_TOKENS * 4
                                        text_splitter = RecursiveCharacterTextSplitter(
                                            chunk_size=char_chunk_size,
                                            chunk_overlap=char_chunk_overlap,
                                            length_function=len,
                                            is_separator_regex=False,
                                        )
                                        logger.info(f"Using character-based splitter with chunk_size={char_chunk_size}, overlap={char_chunk_overlap}.")
                                
                                    all_chunks: List[Document] = []
                                    elements_skipped = 0
                                
                                    for i, element_doc in enumerate(loaded_elements):
                                        if not element_doc.page_content or len(element_doc.page_content.strip()) < MIN_ELEMENT_LENGTH_FOR_CHUNKING:
                                            logger.debug(f"Skipping element {i+1}/{len(loaded_elements)} from {element_doc.metadata.get('file_name', 'N/A')} due to short/empty content.")
                                            elements_skipped += 1
                                            continue
                                        
                                        try:
                                            chunks_from_element = text_splitter.split_documents([element_doc])
                                            if chunks_from_element:
                                                all_chunks.extend(chunks_from_element)
                                                logger.debug(f"Processed element {i+1}/{len(loaded_elements)}. Original length (chars): {len(element_doc.page_content)}. Generated {len(chunks_from_element)} chunk(s).")
                                            else:
                                                logger.debug(f"Element {i+1}/{len(loaded_elements)} from {element_doc.metadata.get('file_name', 'N/A')} resulted in no chunks.")
                                        except Exception as e:
                                            logger.error(f"Error splitting element {i+1} from {element_doc.metadata.get('file_name', 'N/A')}: {e}", exc_info=True)
                                
                                    logger.info(f"Chunking process completed.")
                                    logger.info(f"Original document elements considered for chunking: {len(loaded_elements) - elements_skipped}")
                                    logger.info(f"Elements skipped due to initial short/empty content: {elements_skipped}")
                                    logger.info(f"Total chunks created: {len(all_chunks)}")
                                    return all_chunks
                                
                                
                                if __name__ == "__main__":
                                    logger.info("Running preprocessor.py directly for testing...")
                                    dummy_elements = [
                                        Document(
                                            page_content="This is the first element. It is a moderately long paragraph that should ideally form a single chunk if CHUNK_SIZE_TOKENS is large enough, or be split if it's too long based on token count. It talks about various interesting concepts in computational neuroscience.",
                                            metadata={"source": "test_doc.pdf", "file_name": "test_doc.pdf", "page_number": 1, "category": "NarrativeText"}
                                        ),
                                        Document(page_content="Short.", metadata={"source": "test_doc.pdf", "file_name": "test_doc.pdf", "page_number": 1, "category": "ListItem"}), # Test short skip
                                        Document(
                                            page_content="This is another element, also from page 1. " * 30, # Ensure significant length for splitting
                                            metadata={"source": "test_doc.pdf", "file_name": "test_doc.pdf", "page_number": 1, "category": "NarrativeText"}
                                        ),
                                    ]
                                    logger.info(f"Created {len(dummy_elements)} dummy document elements for preprocessor.py direct test.")
                                    processed_chunks = chunk_all_document_elements(dummy_elements)
                                
                                    if processed_chunks:
                                        logger.info(f"--- Example of first created chunk from preprocessor.py direct test ---")
                                        first_chunk = processed_chunks[0]
                                        logger.info(f"Content snippet: {first_chunk.page_content[:200]}...")
                                        logger.info(f"Metadata: {first_chunk.metadata}")
                                        try:
                                            test_tokenizer = get_tokenizer_for_reference_counting()
                                            logger.info(f"Token count (using 'cl100k_base' for reference): {len(test_tokenizer.encode(first_chunk.page_content))}")
                                        except Exception as e:
                                            logger.warning(f"Could not get reference token count for test print: {e}")
                                    else:
                                        logger.warning("Preprocessor.py direct test: No chunks were created.")
                                    logger.info("Preprocessor.py direct test finished.")
--------------------
                loader.py:
                                # src/cna_rag_agent/data_ingestion/loader.py
                                
                                import os
                                import re
                                import docx
                                from pathlib import Path
                                from typing import List, Dict, Tuple, Optional, Iterator
                                
                                from langchain_core.documents import Document
                                from langchain_community.document_loaders import UnstructuredFileLoader
                                
                                try:
                                    from ..config import RAW_DOCUMENTS_PATH
                                    from ..utils.logging_config import logger
                                except ImportError:
                                    import sys
                                    SRC_DIR = Path(__file__).resolve().parent.parent.parent
                                    if str(SRC_DIR) not in sys.path: sys.path.insert(0, str(SRC_DIR))
                                    from cna_rag_agent.config import RAW_DOCUMENTS_PATH
                                    from cna_rag_agent.utils.logging_config import logger
                                
                                
                                # --- FINAL, CORRECTED PARSER FOR THE HANDBOOK ---
                                def parse_handbook_docx(file_path: Path) -> Iterator[Document]:
                                    """
                                    Parses the structured 'Handbook of CNfA Experimental Paradigms.docx'.
                                    It reads the document paragraph by paragraph, identifies hierarchical headings,
                                    and correctly handles content on the same line as a heading.
                                    """
                                    logger.info(f"Using final, corrected handbook parser for: {file_path.name}")
                                    
                                    category_regex = re.compile(r"^\s*([IVXLCDM]+)\.\s+(.*)")
                                    paradigm_regex = re.compile(r"^\s*(\??[A-Z])\.\s+(.*)")
                                    section_headings = [
                                        "What is Studied", "Why it Matters", "Procedural Summary Table",
                                        "Theoretical Background & Major Explanations", "Example Findings",
                                        "Explanation of Measurables", "Stimuli Description"
                                    ]
                                
                                    doc = docx.Document(file_path)
                                    
                                    current_category = "Uncategorized"
                                    current_paradigm = "Uncategorized"
                                    current_section = "General"
                                    
                                    buffer = []
                                
                                    def yield_buffer() -> Iterator[Document]:
                                        nonlocal buffer
                                        if buffer:
                                            content = "\n".join(buffer).strip()
                                            if content:
                                                metadata = {
                                                    "source": str(file_path.resolve()),
                                                    "file_name": file_path.name,
                                                    "category": current_category,
                                                    "paradigm_name": current_paradigm,
                                                    "paradigm_section": current_section,
                                                    "content_type": "handbook_entry"
                                                }
                                                yield Document(page_content=content, metadata=metadata)
                                            buffer = []
                                
                                    for para in doc.paragraphs:
                                        text = para.text.strip()
                                        if not text:
                                            continue
                                
                                        category_match = category_regex.match(text)
                                        paradigm_match = paradigm_regex.match(text)
                                        
                                        # --- NEW LOGIC TO HANDLE HEADINGS AND CONTENT ON THE SAME LINE ---
                                        found_heading = False
                                        
                                        if category_match:
                                            yield from yield_buffer()
                                            current_category = text
                                            current_paradigm = "General Introduction"
                                            current_section = "Category Title"
                                            found_heading = True
                                            # No content to extract from the category line itself
                                            
                                        elif paradigm_match:
                                            yield from yield_buffer()
                                            current_paradigm = text
                                            current_section = "Paradigm Title"
                                            found_heading = True
                                            # No content to extract from the paradigm line itself
                                            
                                        else:
                                            for heading in section_headings:
                                                if text.startswith(heading):
                                                    yield from yield_buffer()
                                                    current_section = heading
                                                    found_heading = True
                                                    # If there's content after the heading, add it to the new buffer
                                                    content_after_heading = text[len(heading):].strip()
                                                    # Remove leading dashes or colons often found after headings
                                                    if content_after_heading.startswith(('–', '—', ':')):
                                                        content_after_heading = content_after_heading[1:].strip()
                                                    
                                                    if content_after_heading:
                                                        buffer.append(content_after_heading)
                                                    break # Stop checking for other headings
                                        
                                        if not found_heading:
                                            # If the line is not a heading, it's regular content
                                            buffer.append(text)
                                        # --- END OF NEW LOGIC ---
                                
                                    yield from yield_buffer() # Yield any remaining content
                                
                                
                                SUPPORTED_EXTENSIONS: Dict[str, Tuple[type[UnstructuredFileLoader], Dict]] = {
                                    ".pdf": (UnstructuredFileLoader, {"mode": "elements", "strategy": "hi_res"}),
                                    ".docx": (UnstructuredFileLoader, {"mode": "elements", "strategy": "fast"}),
                                    ".doc": (UnstructuredFileLoader, {"mode": "elements", "strategy": "fast"}),
                                    ".txt": (UnstructuredFileLoader, {"mode": "elements"}),
                                }
                                
                                # The rest of the file remains unchanged.
                                def load_single_document(file_path: Path) -> List[Document]:
                                    file_ext = file_path.suffix.lower()
                                    
                                    logger.debug(f"Attempting to load document: {file_path} (extension: {file_ext})")
                                    if file_ext not in SUPPORTED_EXTENSIONS:
                                        logger.warning(f"Unsupported file extension '{file_ext}' for file: {file_path}. Skipping.")
                                        return []
                                
                                    LoaderClass, loader_kwargs = SUPPORTED_EXTENSIONS[file_ext]
                                    loader = LoaderClass(str(file_path), **loader_kwargs)
                                    try:
                                        loaded_docs = loader.load()
                                        logger.info(f"Successfully loaded {len(loaded_docs)} element(s) from: {file_path}")
                                        for doc_element in loaded_docs:
                                            doc_element.metadata["file_name"] = file_path.name
                                            doc_element.metadata["file_path"] = str(file_path.resolve())
                                            if "source" not in doc_element.metadata: doc_element.metadata["source"] = str(file_path.resolve())
                                            if 'page_number' not in doc_element.metadata and 'page' in doc_element.metadata: doc_element.metadata['page_number'] = doc_element.metadata['page']
                                        return loaded_docs
                                    except Exception as e:
                                        logger.error(f"Error loading document {file_path}: {e}", exc_info=True)
                                        return []
                                
                                def load_all_documents(documents_dir: Path = RAW_DOCUMENTS_PATH) -> List[Document]:
                                    if not documents_dir.exists() or not documents_dir.is_dir():
                                        logger.error(f"Documents directory '{documents_dir}' does not exist or is not a directory.")
                                        return []
                                    logger.info(f"Starting to load documents from directory: {documents_dir}")
                                    all_loaded_documents: List[Document] = []
                                    files_processed = 0
                                    files_skipped = 0
                                    
                                    for item in documents_dir.iterdir():
                                        if item.is_file():
                                            files_processed += 1
                                            if item.name == "BEST_Handbook of CNfA Experimental Paradigms.docx":
                                                docs_from_file = list(parse_handbook_docx(item))
                                                if docs_from_file:
                                                    all_loaded_documents.extend(docs_from_file)
                                                else:
                                                    logger.warning(f"Specialized parser for {item.name} yielded no documents.")
                                                    files_skipped += 1
                                            else:
                                                docs_from_file = load_single_document(item)
                                                if docs_from_file:
                                                    all_loaded_documents.extend(docs_from_file)
                                                else:
                                                    files_skipped += 1
                                        elif item.is_dir():
                                            logger.info(f"Found subdirectory '{item.name}'. Recursive loading not implemented. Skipping.")
                                
                                    logger.info(f"Finished loading documents from {documents_dir}.")
                                    logger.info(f"Total files found (top-level): {files_processed}")
                                    logger.info(f"Files successfully processed into elements: {files_processed - files_skipped}")
                                    logger.info(f"Files skipped (unsupported or error): {files_skipped}")
                                    logger.info(f"Total document elements loaded: {len(all_loaded_documents)}")
                                    return all_loaded_documents
                                
                                
                                def get_full_text_for_article(file_path: Path) -> Optional[str]:
                                    logger.info(f"Attempting to load full text for article: {file_path}")
                                    document_elements = load_single_document(file_path)
                                    if not document_elements:
                                        logger.error(f"Failed to load any elements from article: {file_path}")
                                        return None
                                    full_text = "\n\n".join([doc.page_content for doc in document_elements if doc.page_content and doc.page_content.strip()])
                                    if not full_text.strip():
                                        logger.warning(f"Full text for article {file_path} is empty after joining elements.")
                                        return None
                                    logger.info(f"Successfully extracted full text for article: {file_path} (length: {len(full_text)} chars)")
                                    return full_text
                                
                                if __name__ == "__main__":
                                    logger.info("Running loader.py directly for testing...")
                                    test_docs_path = RAW_DOCUMENTS_PATH
                                    
                                    if not any(f for f in test_docs_path.iterdir() if f.is_file() and not f.name.startswith('.') and f.suffix):
                                        dummy_txt_path = test_docs_path / "dummy_loader_test.txt"
                                        with open(dummy_txt_path, "w") as f:
                                            f.write("This is a simple test document for the loader direct run.")
                                        logger.info(f"Created dummy test file for loader.py: {dummy_txt_path}")
                                        
                                    loaded_documents = load_all_documents(test_docs_path)
                                    if loaded_documents:
                                        logger.info(f"load_all_documents test loaded {len(loaded_documents)} elements.")
                                        
                                        handbook_chunk_found = False
                                        for doc in loaded_documents:
                                            if doc.metadata.get("content_type") == "handbook_entry":
                                                logger.info("\n--- Example of a parsed handbook chunk ---")
                                                logger.info(f"Content: {doc.page_content[:200]}...")
                                                logger.info(f"Metadata: {doc.metadata}")
                                                handbook_chunk_found = True
                                                break
                                        if not handbook_chunk_found:
                                            logger.warning("Could not find a chunk parsed from the handbook to display. Ensure the file is in the raw_documents folder.")
                                            
                                    logger.info("Loader.py direct test finished.")
--------------------
            generation/
                __init__.py:
                                
--------------------
                prompts.py:
                                from langchain_core.prompts import PromptTemplate
                                
                                # Basic QA Prompt - instructs the LLM to answer based only on context.
                                QA_TEMPLATE_STR = """You are an expert AI assistant for answering questions based on provided technical documents.
                                Your goal is to provide accurate and concise answers derived *only* from the context below.
                                If the information to answer the question is not in the context, state clearly: "I cannot answer this question based on the provided documents."
                                Do not make up information or answer from outside the given context.
                                
                                Context:
                                {context}
                                
                                Question: {question}
                                
                                Answer:
                                """
                                
                                QA_PROMPT = PromptTemplate.from_template(QA_TEMPLATE_STR)
                                
                                
                                # Add more prompt templates here as needed, for example:
                                # - For the two-step article selection
                                # - For generating the 3-page summaries
                                # - For extracting "stimuli duplication" information
                                
                                THREE_PAGER_SUMMARY_TEMPLATE_STR = """You are a scientific assistant tasked with creating a detailed "3-pager" summary of the following technical article content.
                                The summary should enable another scientist to quickly grasp the article's core concepts and understand how to reproduce its key experiments/stimuli if applicable.
                                
                                Article Content:
                                {article_text}
                                
                                Based *only* on the provided article content, generate a comprehensive summary with the following sections:
                                1.  **Introduction & Problem Statement:** Briefly explain the context and the problem the article addresses.
                                2.  **Key Methodologies:** Describe the main methods, approaches, and techniques used. If mathematical formalisms are central, briefly describe their purpose.
                                3.  **Core Findings & Results:** Summarize the most important outcomes, discoveries, and data presented.
                                4.  **Detailed Experimental Setup / Stimuli Duplication (if applicable):**
                                    * If the article describes experiments or stimuli, extract all relevant details needed for duplication.
                                    * This includes: materials, equipment (and model numbers/versions if provided), reagents, software (and versions if provided), hardware configurations, specific parameters (e.g., concentrations, temperatures, durations, frequencies, visual angles, luminance levels, participant instructions, dataset details and accessibility if mentioned).
                                    * If procedures are detailed, summarize them.
                                    * If this information is not present or not applicable, state "Experimental/stimuli duplication details are not provided or not applicable."
                                5.  **Discussion & Implications:** Briefly discuss the significance of the findings as stated by the authors, limitations mentioned, and potential future work suggested.
                                6.  **Key Figures/Tables to Reference (Optional but helpful):** If specific figures or tables are central to understanding the main points, list them (e.g., "Figure 1 shows...", "Table 2 summarizes...").
                                
                                The overall summary should be detailed yet concise, aiming for the conceptual equivalent of 2-3 pages of text.
                                Focus on extracting factual information relevant to scientific understanding and potential reproduction. Do not add external knowledge.
                                If parts of the article content are unclear or seem incomplete for a section, note that in your summary for that section.
                                
                                Begin Summary:
                                """
                                
                                THREE_PAGER_SUMMARY_PROMPT = PromptTemplate.from_template(THREE_PAGER_SUMMARY_TEMPLATE_STR)
                                
                                # You can also add a specific prompt for the first step of article selection, e.g.,
                                # "Given the user query '{user_query}' and the following article titles and abstracts: {articles_info},
                                # which articles are most relevant to answer the query? List their identifiers."
--------------------
                generator.py:
                                # src/cna_rag_agent/generation/generator.py
                                
                                import asyncio
                                import nest_asyncio
                                
                                nest_asyncio.apply()
                                
                                from typing import Optional, List # Added List for the new function
                                from langchain_google_genai import ChatGoogleGenerativeAI
                                from langchain_core.language_models.chat_models import BaseChatModel
                                from langchain_core.prompts import BasePromptTemplate # For type hinting the prompt
                                
                                # Import from our project
                                try:
                                    from ..config import (
                                        GOOGLE_API_KEY, GEMINI_FLASH_MODEL_NAME, GEMINI_PRO_MODEL_NAME,
                                        LLM_TEMPERATURE_QA, LLM_TEMPERATURE_SUMMARIZATION, # Added SUMMARIZATION temp
                                        LLM_DEFAULT_MAX_OUTPUT_TOKENS
                                    )
                                    from ..utils.logging_config import logger
                                    from .prompts import THREE_PAGER_SUMMARY_PROMPT # Import the new prompt
                                except ImportError:
                                    # ... (fallback import logic - ensure new config vars and prompt are included)
                                    import sys
                                    from pathlib import Path
                                    SRC_DIR = Path(__file__).resolve().parent.parent.parent
                                    if str(SRC_DIR) not in sys.path: sys.path.insert(0, str(SRC_DIR))
                                    from cna_rag_agent.config import (
                                        GOOGLE_API_KEY, GEMINI_FLASH_MODEL_NAME, GEMINI_PRO_MODEL_NAME,
                                        LLM_TEMPERATURE_QA, LLM_TEMPERATURE_SUMMARIZATION,
                                        LLM_DEFAULT_MAX_OUTPUT_TOKENS
                                    )
                                    from cna_rag_agent.utils.logging_config import logger
                                    from cna_rag_agent.generation.prompts import THREE_PAGER_SUMMARY_PROMPT
                                
                                
                                _llm_instance_cache = {}
                                
                                def get_llm(
                                    model_name: str = GEMINI_FLASH_MODEL_NAME,
                                    temperature: float = LLM_TEMPERATURE_QA,
                                    max_output_tokens: Optional[int] = None
                                ) -> Optional[BaseChatModel]:
                                    # ... (get_llm function remains the same as the last correct version) ...
                                    cache_key = (model_name, temperature, max_output_tokens)
                                    if cache_key in _llm_instance_cache: logger.debug(f"Returning cached LLM for: {cache_key}"); return _llm_instance_cache[cache_key]
                                    if not GOOGLE_API_KEY: logger.error("GOOGLE_API_KEY not found. Cannot init LLM."); return None
                                    final_max_tokens = max_output_tokens if max_output_tokens is not None else LLM_DEFAULT_MAX_OUTPUT_TOKENS
                                    try:
                                        logger.info(f"Initializing ChatGoogleGenerativeAI LLM: Model='{model_name}', Temp={temperature}, MaxTokens={final_max_tokens}")
                                        # try: loop = asyncio.get_event_loop(); logger.debug(f"Found existing event loop: {loop}")
                                        # except RuntimeError: logger.info("No current event loop, creating new."); loop = asyncio.new_event_loop(); asyncio.set_event_loop(loop); logger.debug(f"Set new event loop: {loop}")
                                        llm = ChatGoogleGenerativeAI(model=model_name, google_api_key=GOOGLE_API_KEY, temperature=temperature, max_output_tokens=final_max_tokens, convert_system_message_to_human=True)
                                        logger.info("ChatGoogleGenerativeAI LLM initialized successfully."); _llm_instance_cache[cache_key] = llm; return llm
                                    except RuntimeError as e:
                                        if "no current event loop" in str(e): logger.error(f"RuntimeError (event loop issue) for LLM '{model_name}': {e}. Ensure nest_asyncio applied.", exc_info=True)
                                        else: logger.error(f"Failed to init LLM '{model_name}': {e}", exc_info=True)
                                        return None
                                    except Exception as e: logger.error(f"Failed to init LLM '{model_name}': {e}", exc_info=True); return None
                                
                                
                                # --- NEW FUNCTION FOR DETAILED SUMMARY ---
                                def generate_detailed_article_summary(
                                    article_full_text: str,
                                    prompt_template: BasePromptTemplate = THREE_PAGER_SUMMARY_PROMPT,
                                    llm_model_name: str = GEMINI_PRO_MODEL_NAME, # Use Pro model for this complex task
                                    temperature: float = LLM_TEMPERATURE_SUMMARIZATION
                                ) -> Optional[str]:
                                    """
                                    Generates a detailed "3-pager" style summary for a given article's full text.
                                
                                    Args:
                                        article_full_text (str): The complete text content of the article.
                                        prompt_template (BasePromptTemplate): The prompt template to guide the summarization.
                                        llm_model_name (str): The specific Gemini model to use for summarization.
                                        temperature (float): The temperature setting for the LLM.
                                
                                    Returns:
                                        Optional[str]: The generated summary text, or None if an error occurs.
                                    """
                                    if not article_full_text.strip():
                                        logger.warning("Cannot generate summary: article_full_text is empty.")
                                        return None
                                
                                    llm = get_llm(model_name=llm_model_name, temperature=temperature)
                                    if not llm:
                                        logger.error("Cannot generate summary: Failed to initialize LLM.")
                                        return None
                                
                                    logger.info(f"Generating detailed summary for article (text length: {len(article_full_text)} chars) using model {llm_model_name}...")
                                
                                    # Ensure the prompt template expects 'article_text' as an input variable
                                    if "article_text" not in prompt_template.input_variables:
                                        logger.error(f"Prompt template is missing 'article_text' input variable. Current variables: {prompt_template.input_variables}")
                                        return None
                                
                                    formatted_prompt = prompt_template.format(article_text=article_full_text)
                                
                                    try:
                                        # For long article texts, ensure your chosen model (Gemini Pro 1.5) can handle the context window.
                                        # The max_output_tokens for the LLM instance might also need to be sufficient for a "3-pager".
                                        # The default LLM_DEFAULT_MAX_OUTPUT_TOKENS is 8192 which should be plenty for the summary.
                                        response = llm.invoke(formatted_prompt)
                                        summary_text = response.content # Assuming .content holds the text from BaseChatModel
                                        
                                        logger.info(f"Successfully generated detailed summary (length: {len(summary_text)} chars).")
                                        return summary_text
                                    except Exception as e:
                                        logger.error(f"Error during detailed summary generation: {e}", exc_info=True)
                                        return None
                                # --- END NEW FUNCTION ---
                                
                                
                                if __name__ == "__main__":
                                    logger.info("Running generator.py directly for testing LLM initialization and summarization...")
                                    if not GOOGLE_API_KEY:
                                        logger.error("Cannot run test: GOOGLE_API_KEY not set in .env")
                                    else:
                                        # Test LLM getting
                                        qa_llm = get_llm()
                                        if qa_llm: logger.info(f"Successfully got QA LLM instance: {type(qa_llm)}")
                                        else: logger.error("Failed to get QA LLM instance for test.")
                                
                                        # Test Summarization (will make an API call)
                                        logger.info("\n--- Testing Detailed Article Summarization ---")
                                        # Create a short dummy article text for this direct test to save tokens/time
                                        dummy_article = """
                                        Title: The Wonders of Test Data for AI Summarization
                                
                                        Introduction: This document explores the critical role of well-crafted test data. 
                                        It posits that effective AI summarization hinges on diverse and representative input.
                                
                                        Methodology: We employed a qualitative analysis of LLM outputs based on three types of input: 
                                        short narrative texts, structured experimental descriptions, and lengthy theoretical arguments.
                                        The experimental setup involved a 'Model X' with parameters set to temperature 0.5 and top_p 0.9.
                                        Stimuli included visual Gabor patches (contrast 50%, 2 cpd) and auditory sine waves (440Hz, 500ms).
                                
                                        Results: Model X produced coherent summaries for narrative texts. For experimental descriptions, 
                                        it successfully extracted parameters like 'contrast 50%' and '440Hz, 500ms'. 
                                        Theoretical arguments proved most challenging, often missing nuanced implications.
                                
                                        Discussion: The findings suggest that prompts must be highly specific for complex texts. 
                                        Future work should explore chain-of-thought prompting for theoretical summarization. 
                                        The stimuli details were extracted, which is promising for duplication.
                                        """
                                        
                                        summary = generate_detailed_article_summary(dummy_article)
                                        if summary:
                                            logger.info("--- Generated Dummy Article Summary ---")
                                            logger.info(summary)
                                            logger.info("------------------------------------")
                                        else:
                                            logger.error("Failed to generate summary for the dummy article in direct test.")
                                
                                    logger.info("generator.py direct test finished.")
--------------------
